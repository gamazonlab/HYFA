{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train_gtex import *\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.train_utils import forward\n",
    "from src.distributions import *\n",
    "from src.eval_utils import *\n",
    "from src.baselines import *\n",
    "from tqdm import tqdm\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "import blitzgsea as blitz\n",
    "from gseapy.plot import barplot, dotplot\n",
    "from gseapy.plot import gseaplot\n",
    "import gseapy as gp\n",
    "from Bio.KEGG import REST\n",
    "from src.plot_utils import enr_barplot\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import torch\n",
    "import os\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = 'results'\n",
    "MODEL_PATH = 'data/normalised_model_default.pth'\n",
    "GTEX_FILE = 'data/GTEX_data.csv'\n",
    "METADATA_FILE = 'data/GTEx_Analysis_v8_Annotations_SubjectPhenotypesDS.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', dest='config', default='configs/default.yaml', type=str)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Initialise wandb\n",
    "wandb.init(project='multitissue_imputation', config=args.config, mode='disabled')\n",
    "config = wandb.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "adata = GTEx_v8_normalised_adata(file=GTEX_FILE)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = Data.from_datalist\n",
    "\n",
    "# Split train/val/test\n",
    "donors = adata.obs['Participant ID'].values\n",
    "train_donors, test_donors = split_patient_train_test(donors, train_rate=0.8)\n",
    "train_donors, val_donors = split_patient_train_test(train_donors, train_rate=0.75)\n",
    "train_mask = np.isin(donors, train_donors)\n",
    "test_mask = np.isin(donors, test_donors)\n",
    "val_mask = np.isin(donors, val_donors)\n",
    "\n",
    "train_dataset = HypergraphDataset(adata[train_mask], disjoint=True, static=False)\n",
    "val_dataset = HypergraphDataset(adata[val_mask], disjoint=True, static=True)\n",
    "test_dataset = HypergraphDataset(adata[test_mask], static=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use certain GPU\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Select dynamic/static node types\n",
    "config.update({'static_node_types': {'Tissue': (len(adata.obs['Tissue_idx'].unique()), config.d_tissue),\n",
    "                            'metagenes': (config.meta_G, config.d_gene)}}, allow_val_change=True)\n",
    "config.update({'dynamic_node_types': {'Participant ID': (len(adata.obs['Participant ID'].unique()), config.d_patient)}}, allow_val_change=True)\n",
    "\n",
    "# Model\n",
    "config.G = adata.shape[-1]\n",
    "model = HypergraphNeuralNet(config).to(device)  # .double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot tissue embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.params['Tissue'].cpu().detach().numpy()\n",
    "\n",
    "tissue_params_2d = TSNE(n_components=2,\n",
    "                        learning_rate='auto',\n",
    "                        init='random',\n",
    "                        random_state=0).fit_transform(params)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = ['#ffaa56', '#cdad22', '#8fbc8f', '#8b1c62', '#ee6a50', '#ff0000', '#eeee00', '#eeee00', '#eeee00',\n",
    "          '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00',\n",
    "          '#eeee00', '#00cdcd', '#9ac0cd', '#ee82ee', '#cdb79e', '#eec591', '#8b7355', '#8b7355', '#cdaa7d',\n",
    "          '#b452cd', '#7a378b', '#cdb79e', '#cdb79e', '#9acd32', '#cdb79e', '#7A67EE', '#FFD700', '#FFB6C1',\n",
    "          '#CD9B1D', '#B4EEB4', '#D9D9D9', '#3A5FCD', '#1E90FF', '#CDB79E', '#CDB79E', '#FFD39B', '#A6A6A6',\n",
    "          '#008B45', '#EED5D2', '#EED5D2', '#FF00FF']\n",
    "ax = plt.gca()\n",
    "x1, x2 = tissue_params_2d.T\n",
    "ax.scatter(x1, x2,\n",
    "           c=colors,\n",
    "           s=300)\n",
    "\n",
    "for t, i in tissue_dict.items():\n",
    "    x_coord = x1[i]\n",
    "    y_coord = x2[i]\n",
    "    txt = t.replace('_', ' ').replace('Brain', '')\n",
    "        \n",
    "    ax.annotate(txt, (x_coord, y_coord),\n",
    "                textcoords=\"offset points\",  # how to position the text\n",
    "                xytext=(0, 10),  # distance from text to points (x,y)\n",
    "                fontsize=12,\n",
    "                # fontweight='bold',\n",
    "                # rotation=45,\n",
    "                ha='center')\n",
    "plt.axis('off')\n",
    "# plt.title('Tissue embeddings from multi-tissue imputation model', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/tsne_tissue_embeddings.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.params['Tissue'].cpu().detach().numpy()\n",
    "\n",
    "tissue_params_2d = umap.UMAP().fit_transform(params)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = ['#ffaa56', '#cdad22', '#8fbc8f', '#8b1c62', '#ee6a50', '#ff0000', '#eeee00', '#eeee00', '#eeee00',\n",
    "          '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00',\n",
    "          '#eeee00', '#00cdcd', '#9ac0cd', '#ee82ee', '#cdb79e', '#eec591', '#8b7355', '#8b7355', '#cdaa7d',\n",
    "          '#b452cd', '#7a378b', '#cdb79e', '#cdb79e', '#9acd32', '#cdb79e', '#7A67EE', '#FFD700', '#FFB6C1',\n",
    "          '#CD9B1D', '#B4EEB4', '#D9D9D9', '#3A5FCD', '#1E90FF', '#CDB79E', '#CDB79E', '#FFD39B', '#A6A6A6',\n",
    "          '#008B45', '#EED5D2', '#EED5D2', '#FF00FF']\n",
    "ax = plt.gca()\n",
    "x1, x2 = tissue_params_2d.T\n",
    "ax.scatter(x1, x2,\n",
    "           c=colors,\n",
    "           s=300)\n",
    "\n",
    "for t, i in tissue_dict.items():\n",
    "    x_coord = x1[i]\n",
    "    y_coord = x2[i]\n",
    "    txt = t.replace('_', ' ').replace('Brain', '')\n",
    "        \n",
    "    ax.annotate(txt, (x_coord, y_coord),\n",
    "                textcoords=\"offset points\",  # how to position the text\n",
    "                xytext=(0, 10),  # distance from text to points (x,y)\n",
    "                fontsize=12,\n",
    "                # fontweight='bold',\n",
    "                # rotation=45,\n",
    "                ha='center')\n",
    "\n",
    "    \n",
    "plt.axis('off')\n",
    "# plt.title('Tissue embeddings from multi-tissue imputation model', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/umap_tissue_embeddings.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: predictions validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out, node_features = forward(d, model, device, preprocess_fn=None, use_latent_mean=True)\n",
    "    x_pred = torch.distributions.normal.Normal(loc=out['px_rate'], scale=out['px_r']).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9228\n",
    "plt.scatter(d.x_target[:, idx].cpu().numpy(), x_pred[:, idx].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corr = True\n",
    "\n",
    "def rho(x, x_pred):\n",
    "    return np.mean(pearson_correlation_score(x, x_pred, sample_corr=sample_corr))\n",
    "metric_fns = [rho]\n",
    "    \n",
    "model.eval()\n",
    "score_fn = pearson_correlation_score\n",
    "\n",
    "validate = False\n",
    "source_tissues = ['Whole_Blood']    # , 'Skin_Sun_Epsd', 'Skin_Not_Sun_Epsd', 'Adipose_Subcutaneous'\n",
    "target_tissues = [t for t in adata.obs['Tissue'].unique() if t not in source_tissues]\n",
    "\n",
    "results_df = pd.DataFrame([], columns=['score', 'source', 'target', 'method'])\n",
    "for tt in tqdm(target_tissues):\n",
    "    # Name source and target tissues\n",
    "    source_name = ', '.join(source_tissues)\n",
    "    target_name = tt.replace('_', ' ')\n",
    "    # print(tt)\n",
    "    \n",
    "    # Create datasets\n",
    "    aux_train_dataset = HypergraphDataset(adata[train_mask],\n",
    "                                      obs_source={'Tissue': source_tissues},\n",
    "                                      obs_target={'Tissue': [tt]})\n",
    "    source_donor_ids = aux_train_dataset.adata_source.obs['Participant ID']\n",
    "    target_donor_ids = aux_train_dataset.adata_target.obs['Participant ID']\n",
    "    assert (source_donor_ids.values == target_donor_ids.values).all()\n",
    "    \n",
    "    aux_val_dataset = HypergraphDataset(adata[val_mask],\n",
    "                                      obs_source={'Tissue': source_tissues},\n",
    "                                      obs_target={'Tissue': [tt]})\n",
    "    source_donor_ids = aux_val_dataset.adata_source.obs['Participant ID']\n",
    "    target_donor_ids = aux_val_dataset.adata_target.obs['Participant ID']\n",
    "    assert (source_donor_ids.values == target_donor_ids.values).all()\n",
    "    \n",
    "    aux_test_dataset = HypergraphDataset(adata[test_mask],\n",
    "                                      obs_source={'Tissue': source_tissues},\n",
    "                                      obs_target={'Tissue': [tt]})\n",
    "    source_donor_ids = aux_test_dataset.adata_source.obs['Participant ID']\n",
    "    target_donor_ids = aux_test_dataset.adata_target.obs['Participant ID']\n",
    "    assert (source_donor_ids.values == target_donor_ids.values).all()\n",
    "\n",
    "    # Prepare source expression data\n",
    "    x_train_ = aux_train_dataset.adata_source.layers['x'].toarray()\n",
    "    x_train_covs = aux_train_dataset.adata_source.obsm['Participant ID_feat'].toarray()\n",
    "    x_val_ = aux_val_dataset.adata_source.layers['x'].toarray()\n",
    "    x_val_covs = aux_val_dataset.adata_source.obsm['Participant ID_feat'].toarray()\n",
    "    x_test_ = aux_test_dataset.adata_source.layers['x'].toarray()\n",
    "    x_test_covs = aux_test_dataset.adata_source.obsm['Participant ID_feat'].toarray()\n",
    "    \n",
    "    y_train = aux_train_dataset.adata_target.layers['x'].toarray()\n",
    "    y_val = aux_val_dataset.adata_target.layers['x'].toarray()\n",
    "    y_test = aux_test_dataset.adata_target.layers['x'].toarray()\n",
    "    \n",
    "    # Append donor metadata\n",
    "    x_train_aux = aux_train_dataset.adata_source.obsm['Participant ID_feat'].toarray()\n",
    "    x_val_aux = aux_val_dataset.adata_source.obsm['Participant ID_feat'].toarray()\n",
    "    x_test_aux = aux_test_dataset.adata_source.obsm['Participant ID_feat'].toarray()\n",
    "    x_train = np.concatenate((x_train_, x_train_aux), axis=-1)\n",
    "    x_val = np.concatenate((x_val_, x_val_aux), axis=-1)\n",
    "    x_test = np.concatenate((x_test_, x_test_aux), axis=-1)\n",
    "    \n",
    "    if validate:\n",
    "        x_test = x_val\n",
    "        y_test = y_val\n",
    "        x_test_ = x_val_\n",
    "        x_test_covs = x_val_covs\n",
    "    \n",
    "    # Blood surrogate baseline\n",
    "    sample_scores = score_fn(y_test, x_test_, sample_corr=sample_corr)\n",
    "    \n",
    "    # Append results\n",
    "    scores = sample_scores\n",
    "    df_ = pd.DataFrame({'score': scores,\n",
    "                        'source': [source_name] * len(scores),\n",
    "                        'target': [target_name] * len(scores),\n",
    "                        'method': ['blood surrogate'] * len(scores)})\n",
    "    results_df = pd.concat([results_df, df_])\n",
    "    \n",
    "    # Mean baseline\n",
    "    if sample_corr:  # Not defined when sampling units are genes\n",
    "        means = y_train.mean(axis=0)\n",
    "        y_test_pred = np.repeat(means[None, :], y_test.shape[0], axis=0)\n",
    "        sample_scores = score_fn(y_test, y_test_pred, sample_corr=sample_corr)\n",
    "\n",
    "        # Append results\n",
    "        scores = sample_scores\n",
    "        df_ = pd.DataFrame({'score': scores,\n",
    "                            'source': [source_name] * len(scores),\n",
    "                            'target': [target_name] * len(scores),\n",
    "                            'method': ['mean'] * len(scores)})\n",
    "        results_df = pd.concat([results_df, df_])\n",
    "    \n",
    "    # KNN baseline\n",
    "    x_train_knn = np.concatenate((x_train_, y_train), axis=-1)\n",
    "    test_nans = np.full((x_test_.shape[0], y_train.shape[1]), np.nan)\n",
    "    x_test_knn = np.concatenate((x_test_, test_nans), axis=-1)\n",
    "    x_knn = np.concatenate((x_train_knn, x_test_knn), axis=0)\n",
    "\n",
    "    x_knn_covs = np.concatenate((x_train_covs, x_test_covs), axis=0)\n",
    "    knn_imp = impute_knn(x_knn, covariates=x_knn_covs, k=20)\n",
    "    knn_imp_ = knn_imp[x_train_.shape[0]:, x_train_.shape[1]:]\n",
    "    sample_scores = score_fn(y_test, knn_imp_, sample_corr=sample_corr)\n",
    "    \n",
    "    # Append results\n",
    "    scores = sample_scores\n",
    "    df_ = pd.DataFrame({'score': scores,\n",
    "                        'source': [source_name] * len(scores),\n",
    "                        'target': [target_name] * len(scores),\n",
    "                        'method': ['kNN'] * len(scores)})\n",
    "    results_df = pd.concat([results_df, df_])\n",
    "    \n",
    "    # TEEBoT baseline.\n",
    "    y_test_pred = PCA_linear_regression_baseline(x_train, y_train, x_test)\n",
    "    \n",
    "    sample_scores = score_fn(y_test, y_test_pred, sample_corr=sample_corr)\n",
    "    \n",
    "    # Append results\n",
    "    scores = sample_scores\n",
    "    df_ = pd.DataFrame({'score': scores,\n",
    "                        'source': [source_name] * len(scores),\n",
    "                        'target': [target_name] * len(scores),\n",
    "                        'method': ['TEEBoT'] * len(scores)})\n",
    "    results_df = pd.concat([results_df, df_])\n",
    "    \n",
    "    # Hypergraph baseline\n",
    "    aux_train_loader = DataLoader(aux_train_dataset, batch_size=config.batch_size, collate_fn=collate_fn, shuffle=True, drop_last=True)\n",
    "    aux_val_loader = DataLoader(aux_val_dataset, batch_size=len(aux_val_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "    aux_test_loader = DataLoader(aux_test_dataset, batch_size=len(aux_test_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    # Compute predictions and score\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if validate:\n",
    "            d = next(iter(aux_val_loader))\n",
    "        else:\n",
    "            d = next(iter(aux_test_loader))\n",
    "\n",
    "        out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "        y_test_pred = out['px_rate'].cpu().numpy()  # torch.distributions.normal.Normal(loc=out['px_rate'], scale=out['px_r']).mean.cpu().numpy()\n",
    "        y_test_ = d.x_target.cpu().numpy()\n",
    "    assert np.allclose(y_test_, y_test)\n",
    "\n",
    "    sample_scores = score_fn(y_test, y_test_pred, sample_corr=sample_corr)\n",
    "\n",
    "    # Append results\n",
    "    scores = sample_scores\n",
    "    df_ = pd.DataFrame({'score': scores,\n",
    "                        'source': [source_name] * len(scores),\n",
    "                        'target': [target_name] * len(scores),\n",
    "                        'method': ['HYFA (blood)'] * len(scores)})\n",
    "    results_df = pd.concat([results_df, df_])\n",
    "\n",
    "    # Hypergraph baseline (all tissues)\n",
    "    # Select same set of individuals\n",
    "    aux_val_dataset = HypergraphDataset(adata[val_mask],\n",
    "                                      obs_source={'Participant ID': list(aux_val_dataset.donor_map.values()), \n",
    "                                                 'Tissue': [t for t in adata.uns['Tissue_dict'].keys() if t != tt]},\n",
    "                                      obs_target={'Tissue': [tt]})\n",
    "    aux_test_dataset = HypergraphDataset(adata[test_mask],\n",
    "                                      obs_source={'Participant ID': list(aux_test_dataset.donor_map.values()),\n",
    "                                                 'Tissue': [t for t in adata.uns['Tissue_dict'].keys() if t != tt]},\n",
    "                                      obs_target={'Tissue': [tt]})\n",
    "    aux_val_loader = DataLoader(aux_val_dataset, batch_size=len(aux_val_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "    aux_test_loader = DataLoader(aux_test_dataset, batch_size=len(aux_test_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    # Compute predictions and score\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if validate:\n",
    "            d = next(iter(aux_val_loader))\n",
    "        else:\n",
    "            d = next(iter(aux_test_loader))\n",
    "\n",
    "        out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "        y_test_pred = out['px_rate'].cpu().numpy()  # torch.distributions.normal.Normal(loc=out['px_rate'], scale=out['px_r']).mean.cpu().numpy()\n",
    "        y_test_ = d.x_target.cpu().numpy()\n",
    "\n",
    "    sample_scores = score_fn(y_test, y_test_pred, sample_corr=sample_corr)\n",
    "\n",
    "    # Append results\n",
    "    scores = sample_scores\n",
    "    df_ = pd.DataFrame({'score': scores,\n",
    "                        'source': [source_name] * len(scores),\n",
    "                        'target': [target_name] * len(scores),\n",
    "                        'method': ['HYFA (all)'] * len(scores)})\n",
    "    results_df = pd.concat([results_df, df_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.6)\n",
    "plt.figure(figsize = (20, 3))\n",
    "sns.barplot(y='score', x='target', hue='method',\n",
    "            data=results_df[(results_df['method'] == 'HYFA (blood)') | (results_df['method'] == 'TEEBoT')], order=np.unique(results_df['target']),\n",
    "            )  # capsize = 0.1\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Prediction performance with whole blood as source')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation');\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -1), fancybox=True, shadow=True, ncol=2);\n",
    "# plt.savefig(f'{RESULTS_DIR}/comparison_scores_pertissue_blood_sample_corr{sample_corr}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "results_df['score'] = pd.to_numeric(results_df['score'])\n",
    "ranks = results_df.groupby('method')['score'].median().fillna(0).sort_values().index\n",
    "\n",
    "sns.boxplot(x='method', y='score', data=results_df, order=ranks)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation')\n",
    "plt.title('Performance with whole blood as source');\n",
    "# plt.savefig(f'{RESULTS_DIR}/aggregated_scores_blood_sample_corr{sample_corr}.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "results_df['score'] = pd.to_numeric(results_df['score'])\n",
    "ranks = results_df.groupby('method')['score'].median().fillna(0).sort_values().index\n",
    "\n",
    "sns.boxplot(x='method', y='score', data=results_df, order=ranks)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation')\n",
    "plt.title('Performance with whole blood as source');\n",
    "# plt.savefig(f'{RESULTS_DIR}/aggregated_scores_blood_sample_corr{sample_corr}.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with TEEBoT across multiple tissues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_dict = adata.uns['Tissue_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-load all data for kNN imputation\n",
    "train_loader_all = DataLoader(train_dataset, batch_size=len(train_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "d = next(iter(train_loader_all))\n",
    "y_observed = np.full((len(train_dataset), len(tissue_dict), config.G), np.nan)\n",
    "y_observed[d.source['Participant ID'].numpy(), d.source['Tissue'].numpy(), :] = d.x_source\n",
    "y_covs = d.node_features['Participant ID'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corr = True\n",
    "score_fn = pearson_correlation_score\n",
    "\n",
    "source_tissues = ['Whole_Blood', 'Skin_Sun_Epsd', 'Skin_Not_Sun_Epsd', 'Adipose_Subcutaneous']  \n",
    "ttissues = set(tissue_dict.keys())\n",
    "ttissues = [t for t in ttissues if t not in source_tissues]\n",
    "source_name = 'Accessible tissues'\n",
    "\n",
    "results_df = pd.DataFrame([], columns=['score', 'source', 'target', 'method'])\n",
    "\n",
    "for t in ttissues:\n",
    "    print(', '.join(source_tissues), 'to', t)\n",
    "    target_tissues = [t]\n",
    "    target_name = t.replace('_', ' ')\n",
    "    \n",
    "    # Train set\n",
    "    split_mask = train_mask\n",
    "    valid_donors = []\n",
    "    donors = adata[split_mask].obs['Participant ID'].unique()\n",
    "    for donor in donors:\n",
    "        donor_mask = adata[split_mask].obs['Participant ID'] == donor\n",
    "        all_tissues_collected = all([t in adata[split_mask].obs[donor_mask]['Tissue'].values for t in source_tissues + target_tissues])\n",
    "        if all_tissues_collected:\n",
    "            valid_donors.append(donor)\n",
    "    aux_train_dataset = HypergraphDataset(adata[split_mask],\n",
    "                                    obs_source={'Tissue': source_tissues, 'Participant ID': valid_donors},\n",
    "                                    obs_target={'Tissue': target_tissues, 'Participant ID': valid_donors},\n",
    "                                    static=True, \n",
    "                                    verbose=True)\n",
    "    aux_train_loader = DataLoader(aux_train_dataset, batch_size=len(aux_train_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    # Eval set\n",
    "    split_mask = test_mask\n",
    "    valid_donors = []\n",
    "    donors = adata[split_mask].obs['Participant ID'].unique()\n",
    "    for donor in donors:\n",
    "        donor_mask = adata[split_mask].obs['Participant ID'] == donor\n",
    "        all_tissues_collected = all([t in adata[split_mask].obs[donor_mask]['Tissue'].values for t in source_tissues + target_tissues])\n",
    "        if all_tissues_collected:\n",
    "            valid_donors.append(donor)\n",
    "    \n",
    "    aux_val_dataset = HypergraphDataset(adata[split_mask],\n",
    "                                    obs_source={'Tissue': source_tissues, 'Participant ID': valid_donors},\n",
    "                                    obs_target={'Tissue': target_tissues, 'Participant ID': valid_donors},\n",
    "                                    static=True, \n",
    "                                    verbose=True)\n",
    "    aux_val_loader = DataLoader(aux_val_dataset, batch_size=len(aux_val_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    it = iter(aux_val_loader)\n",
    "    val_d = next(it)\n",
    "    patients_source_val = val_d.source['Participant ID'].cpu().numpy()\n",
    "    \n",
    "    print(source_tissues, target_name, len(np.unique(patients_source_val)))\n",
    "    if len(np.unique(patients_source_val)) >= 25:  # combinations with >= 25 patients\n",
    "        # Reshape and concatenate multiple tissues\n",
    "        it = iter(aux_train_loader)\n",
    "        d = next(it)\n",
    "        x_source = d.x_source.reshape(-1, len(source_tissues)* d.x_source.shape[-1])\n",
    "        x_target = d.x_target  # .reshape(-1, len(source_tissues)* d.x_source.shape[-1])\n",
    "        x_source_val = val_d.x_source.reshape(-1, len(source_tissues) * val_d.x_source.shape[-1])\n",
    "        x_target_val = val_d.x_target # .reshape(-1, len(source_tissues) * val_d.x_source_val.shape[-1])\n",
    "        x_source_covs = d.source_features['Participant ID'].cpu().numpy()\n",
    "        x_source_val_covs  = val_d.source_features['Participant ID'].cpu().numpy()\n",
    "        x_source_covs = x_source_covs.reshape(-1, len(source_tissues), x_source_covs.shape[-1])[:, 0, :]\n",
    "        x_source_val_covs = x_source_val_covs.reshape(-1, len(source_tissues), x_source_val_covs.shape[-1])[:, 0, :]\n",
    "        \n",
    "        # Blood surrogate baseline\n",
    "        blood_source_mask = val_d.source['Tissue'] == 48\n",
    "        scores = score_fn(x_target_val.numpy(), val_d.x_source[blood_source_mask].numpy(), sample_corr=sample_corr)\n",
    "        print(f'Blood surrogate baseline: \\n Mean score: {scores.mean()}')\n",
    "\n",
    "        # Append results\n",
    "        df_ = pd.DataFrame({'score': scores,\n",
    "                            'source': [source_name] * len(scores),\n",
    "                            'target': [target_name] * len(scores),\n",
    "                            'method': ['Blood surrogate'] * len(scores)})\n",
    "        results_df = pd.concat([results_df, df_])\n",
    "\n",
    "        # Mean baseline\n",
    "        if sample_corr:\n",
    "            means = d.x_target.mean(axis=0).numpy()\n",
    "            y_test_pred = np.repeat(means[None, :], x_target_val.shape[0], axis=0)\n",
    "            scores = score_fn(x_target_val.numpy(), y_test_pred, sample_corr=sample_corr)\n",
    "            print(f'Mean baseline: \\n Mean score: {scores.mean()}')\n",
    "\n",
    "            # Append results\n",
    "            df_ = pd.DataFrame({'score': scores,\n",
    "                                'source': [source_name] * len(scores),\n",
    "                                'target': [target_name] * len(scores),\n",
    "                                'method': ['mean'] * len(scores)})\n",
    "            results_df = pd.concat([results_df, df_])\n",
    "\n",
    "        # KNN baseline\n",
    "        x_train_knn = np.concatenate((x_source, x_target), axis=-1)\n",
    "        test_nans = np.full((x_source_val.shape[0], x_target.shape[1]), np.nan)\n",
    "        x_test_knn = np.concatenate((x_source_val, test_nans), axis=-1)\n",
    "        x_knn = np.concatenate((x_train_knn, x_test_knn), axis=0)\n",
    "\n",
    "        x_knn_covs = np.concatenate((x_source_covs, x_source_val_covs), axis=0)\n",
    "        knn_imp = impute_knn(x_knn, covariates=x_knn_covs, k=20)\n",
    "        knn_imp_ = knn_imp[x_source.shape[0]:, x_source.shape[1]:]\n",
    "        scores = score_fn(x_target_val.numpy(), knn_imp_, sample_corr=sample_corr)\n",
    "        print(f'kNN baseline: \\n Mean score: {scores.mean()}')\n",
    "\n",
    "        # Append results\n",
    "        df_ = pd.DataFrame({'score': scores,\n",
    "                            'source': [source_name] * len(scores),\n",
    "                            'target': [target_name] * len(scores),\n",
    "                            'method': ['kNN'] * len(scores)})\n",
    "        results_df = pd.concat([results_df, df_])\n",
    "        \n",
    "        # TEEBoT\n",
    "        x_target_pred = PCA_linear_regression_baseline(x_source.numpy(), x_target.numpy(), x_source_val.numpy(),\n",
    "                                                       x_source_covs=x_source_covs,\n",
    "                                                       x_source_test_covs=x_source_val_covs)\n",
    "        scores = score_fn(x_target_val.numpy(), x_target_pred, sample_corr=sample_corr)\n",
    "        print(f'TEEBoT regression baseline: \\n Mean score: {scores.mean()}')\n",
    "\n",
    "        # Store results\n",
    "        df_ = pd.DataFrame({'score': scores,\n",
    "                            'source': [source_name] * len(scores),\n",
    "                            'target': [target_name] * len(scores),\n",
    "                            'method': ['TEEBoT'] * len(scores)})\n",
    "        results_df = pd.concat([results_df, df_])\n",
    "\n",
    "        # Hypergraph\n",
    "        with torch.no_grad():\n",
    "            d = next(iter(aux_val_loader))\n",
    "            out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "            y_pred = out['px_rate'].cpu().numpy()\n",
    "            y_ = d.x_target.cpu().numpy()\n",
    "        assert np.allclose(y_, x_target_val)\n",
    "\n",
    "        scores = score_fn(x_target_val.numpy(), y_pred, sample_corr=sample_corr)\n",
    "        print(f'Hypergraph neural network (accessible): \\n Mean score: {scores.mean()}')\n",
    "\n",
    "        # Store results\n",
    "        df_ = pd.DataFrame({'score': scores,\n",
    "                            'source': [source_name] * len(scores),\n",
    "                            'target': [target_name] * len(scores),\n",
    "                            'method': ['HYFA (accessible)'] * len(scores)})\n",
    "        results_df = pd.concat([results_df, df_])\n",
    "        \n",
    "        # Hypergraph baseline (all tissues)\n",
    "        # Select same set of individuals\n",
    "        aux_val_dataset_ = HypergraphDataset(adata[split_mask],\n",
    "                                             obs_source={'Participant ID': list(aux_val_dataset.donor_map.values()),\n",
    "                                                         'Tissue': [k for k in adata.uns['Tissue_dict'].keys() if k != t]},\n",
    "                                             obs_target={'Tissue': [t]},\n",
    "                                             static=True)\n",
    "        aux_val_loader_ = DataLoader(aux_val_dataset_, batch_size=len(aux_val_dataset_), collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "        # Compute predictions and score\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            d = next(iter(aux_val_loader_))\n",
    "\n",
    "            out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "            y_val_pred = out['px_rate'].cpu().numpy()  # torch.distributions.normal.Normal(loc=out['px_rate'], scale=out['px_r']).mean.cpu().numpy()\n",
    "            y_test_ = d.x_target.cpu().numpy()\n",
    "\n",
    "        scores = score_fn(x_target_val.numpy(), y_val_pred, sample_corr=sample_corr)\n",
    "        print(f'Hypergraph neural network (all): \\n Mean score: {scores.mean()}')\n",
    "\n",
    "        # Append results\n",
    "        df_ = pd.DataFrame({'score': scores,\n",
    "                            'source': [source_name] * len(scores),\n",
    "                            'target': [target_name] * len(scores),\n",
    "                            'method': ['HYFA (all)'] * len(scores)})\n",
    "        results_df = pd.concat([results_df, df_])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.6)\n",
    "plt.figure(figsize = (20, 4))\n",
    "sns.barplot(y='score', x='target', hue='method', data=results_df[(results_df['method'] == 'HYFA (accessible)') | (results_df['method'] == 'TEEBoT')], order=np.unique(results_df['target']))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Prediction performance with accessible tissues as source tissues (whole blood, skin, and adipose subcutaneous)')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation');\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.75), fancybox=True, shadow=True, ncol=2)\n",
    "# plt.savefig(f'{RESULTS_DIR}/comparison_accessible_scores_valtest_sample_corr{sample_corr}.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "results_df['score'] = pd.to_numeric(results_df['score'])\n",
    "ranks = results_df.groupby('method')['score'].median().fillna(0).sort_values().index\n",
    "\n",
    "sns.boxplot(x='method', y='score', data=results_df, order=ranks)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation')\n",
    "plt.title('Performance with accessible tissues as source')\n",
    "# plt.ylim((-0.75, 0.85))\n",
    "# plt.savefig(f'{RESULTS_DIR}/aggregated_scores_test_accessible_sample_corr{sample_corr}.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'{RESULTS_DIR}/results_pertissue_test_sources_accessible.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_means = results_df[results_df['method'] == 'TEEBoT'].groupby('target')['score'].mean()\n",
    "ours_means = results_df[results_df['method'] == 'HYFA (accessible)'].groupby('target')['score'].mean()\n",
    "(ours_means > baseline_means).sum(), ours_means.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase in performance when multiple accessible tissues are used as source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fn = pearson_correlation_score\n",
    "\n",
    "source_tissues = ['Whole_Blood', 'Skin_Sun_Epsd', 'Skin_Not_Sun_Epsd', 'Adipose_Subcutaneous']\n",
    "source_tissues_idxs = [tissue_dict[t] for t in source_tissues]\n",
    "names = [t.replace('_', ' ') for t in source_tissues] + ['Accessible All']\n",
    "\n",
    "scores_col = []\n",
    "source_col = []\n",
    "target_col = []\n",
    "\n",
    "# Target tissue\n",
    "ttissues = set(tissue_dict.keys()) # - set(['Testis', 'Cells_Cultured'])\n",
    "ttissues = [t for t in ttissues if t not in source_tissues]\n",
    "\n",
    "for t in ttissues:\n",
    "    target_tissues = [t]\n",
    "    target_name = t.replace('_', ' ')\n",
    "    \n",
    "    # Get data\n",
    "    # split_mask = val_mask\n",
    "    split_mask = np.logical_or(val_mask, test_mask)\n",
    "    \n",
    "    donors = adata[split_mask].obs['Participant ID'].unique()\n",
    "\n",
    "    valid_donors = []\n",
    "    for donor in donors:\n",
    "        donor_mask = adata[split_mask].obs['Participant ID'] == donor\n",
    "        all_tissues_collected = all([t in adata[split_mask].obs[donor_mask]['Tissue'].values for t in source_tissues + target_tissues])\n",
    "        if all_tissues_collected:\n",
    "            valid_donors.append(donor)\n",
    "    \n",
    "    aux_dataset = HypergraphDataset(adata[split_mask],\n",
    "                                    obs_source={'Tissue': source_tissues, 'Participant ID': valid_donors},\n",
    "                                    obs_target={'Tissue': target_tissues, 'Participant ID': valid_donors},\n",
    "                                    static=True, \n",
    "                                    verbose=True)\n",
    "    print(len(aux_dataset))\n",
    "    aux_loader = DataLoader(aux_dataset, batch_size=len(aux_dataset), collate_fn=collate_fn, shuffle=False)\n",
    "    \n",
    "    it = iter(aux_loader)\n",
    "    d = next(it)\n",
    "    patients_source = d.source['Participant ID']\n",
    "    tissues_source = d.source['Tissue']\n",
    "    \n",
    "    print(source_tissues, target_name, len(np.unique(patients_source)))\n",
    "    if len(np.unique(patients_source)) >= 25:\n",
    "        # Evaluate performance when increasingly adding more tissue types\n",
    "        cum_source_tissues_idxs = []\n",
    "        # selected_tissues = [[tissue_dict[t]] for t in source_tissues] + [[tissue_dict[t] for t in source_tissues]]\n",
    "        selected_tissues = [[t] for t in source_tissues] + [source_tissues]\n",
    "        print(selected_tissues)\n",
    "        \n",
    "        for source_t, name in zip(selected_tissues, names):\n",
    "            cum_source_tissues_idxs = source_t\n",
    "            \n",
    "            # print(source_t, target_tissues)\n",
    "            # Select samples from subset of individuals having all selected tissues\n",
    "            aux_dataset_ = HypergraphDataset(adata = aux_dataset.adata_source,\n",
    "                                             adata_target = aux_dataset.adata_target,\n",
    "                                             obs_source={'Tissue': source_t},\n",
    "                                             obs_target={'Tissue': target_tissues},\n",
    "                                             static=True)\n",
    "            aux_loader_ = DataLoader(aux_dataset_, batch_size=len(aux_dataset_), collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                d = next(iter(aux_loader_))\n",
    "                out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "                y_pred = out['px_rate'].cpu().numpy()\n",
    "                y_ = d.x_target.cpu().numpy()\n",
    "                \n",
    "            gene_scores = score_fn(y_, y_pred)\n",
    "            sample_scores = score_fn(y_, y_pred, sample_corr=True)\n",
    "            print(f'Hypergraph neural network: \\n Mean score per gene: {gene_scores.mean()}. Mean score per sample: {sample_scores.mean()}')\n",
    "\n",
    "            scores = sample_scores\n",
    "            scores_col.extend(scores)\n",
    "            source_col.extend([name] * len(scores))\n",
    "            target_col.extend([target_name] * len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(results_df[results_df['source'] == 'Accessible All']['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'score': scores_col, 'source': source_col, 'target': target_col})\n",
    "mid_point = 19\n",
    "tt_1 = sorted(results_df['target'].unique())[:mid_point]\n",
    "tt_2 = sorted(results_df['target'].unique())[mid_point:]\n",
    "results_df_1 = results_df[results_df['target'].isin(tt_1)]\n",
    "results_df_2 = results_df[results_df['target'].isin(tt_2)]\n",
    "\n",
    "sns.set(font_scale = 1.6)\n",
    "plt.figure(figsize = (20, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(y='score', x='target', hue='source', data=results_df_1, order=np.unique(results_df_1['target']))\n",
    "plt.legend([], [], frameon=False)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Prediction performance with accessible tissues as source')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation');\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(y='score', x='target', hue='source', data=results_df_2, order=np.unique(results_df_2['target']))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "# plt.legend(bbox_to_anchor=(1.02, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.title('Prediction performance with accessible tissues as source')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Pearson correlation');\n",
    "plt.tight_layout(pad=1.0)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -1), fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "# plt.savefig(f'figures/scores_pertissue_HYFA_accessible.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'{RESULTS_DIR}/results_pertissue_HYFA_sources_accessible.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metagene-factors GSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = blitz.enrichr.get_library('KEGG_2021_Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metagene_w = model.metagenes_encoder.encoder[0].weight.detach().cpu().numpy()\n",
    "metagene_w = metagene_w.reshape((config.meta_G, -1, config.G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = config.d_edge_attr\n",
    "n_metagenes = config.meta_G\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "for f in range(n_factors):\n",
    "    print(f'Factor={f}')\n",
    "    for m in range(n_metagenes):\n",
    "        gene_idxs = np.argsort(metagene_w[m, f, :])[::-1]\n",
    "        gene_names = adata.var['Symbol'][gene_idxs].values\n",
    "        gene_values = metagene_w[m, f, gene_idxs]\n",
    "        signature = pd.DataFrame({0: gene_names, 1: gene_values})\n",
    "        \n",
    "        result = blitz.gsea(signature, library, permutations=2000, signature_cache=True)\n",
    "        result['Factor'] = f\n",
    "        result['Metagene'] = m\n",
    "        results_df = pd.concat([results_df, result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(f'{RESULTS_DIR}/blitz_gsea_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_results_df = results_df[results_df['fdr'] < 0.05]\n",
    "significant_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "plt.figure(figsize=(20, 4))\n",
    "ax = plt.gca()\n",
    "cmap = plt.get_cmap('tab10')\n",
    "\n",
    "n_metagenes = 50\n",
    "n_factors = 99\n",
    "for f in range(n_factors):\n",
    "    df = results_df[results_df['Factor'] == f]\n",
    "    for x in range(n_metagenes):\n",
    "        df_meta = df[df['Metagene'] == x]\n",
    "        y = -np.log10(df_meta['fdr'].values)\n",
    "        x_pos = [f + (x/n_metagenes)] * len(y)\n",
    "        ax.scatter(x_pos, y, s=1, color=cmap(f % 10))\n",
    "plt.xlabel('Factor')\n",
    "plt.ylabel('$-\\log_{10}(q)$')\n",
    "plt.title('All human pathways (KEGG)')\n",
    "plt.xticks(list(range(n_factors)))\n",
    "plt.xticks(rotation = 90)\n",
    "plt.xlim((-1, 99))\n",
    "# plt.axhline(y = -np.log10(0.05), color = 'gray', linestyle = '--', linewidth=1)\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/manhattan_blitzgsea.pdf', bbox_inches='tight');\n",
    "# plt.savefig('overleaf/figures/manhattan_blitzgsea.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "plt.figure(figsize=(20, 4))\n",
    "ax = plt.gca()\n",
    "cmap = plt.get_cmap('tab10')\n",
    "\n",
    "n_metagenes = 50\n",
    "n_factors = 99\n",
    "for f in range(n_factors):\n",
    "    df = results_df[results_df['Factor'] == f]\n",
    "    for x in range(n_metagenes):\n",
    "        df_meta = df[df['Metagene'] == x]\n",
    "        y = -np.log10(df_meta['fdr'].values)\n",
    "        x_pos = [x + (f/n_factors)] * len(y)\n",
    "        ax.scatter(x_pos, y, s=1, color=cmap(x % 10))\n",
    "plt.xlabel('Metagene')\n",
    "plt.ylabel('$-\\log_{10}(q)$')\n",
    "plt.title('All human pathways (KEGG)')\n",
    "locs = list(range(n_metagenes))\n",
    "# plt.xticks(locs, rotation = 90)\n",
    "plt.gca().set_xticklabels('')\n",
    "plt.gca().set_xticks(np.array(locs) + 0.5, minor=True)\n",
    "plt.gca().set_xticklabels(locs, minor=True)\n",
    "plt.xlim((-1, 51))\n",
    "# plt.axhline(y = -np.log10(0.05), color = 'gray', linestyle = '--', linewidth=1)\n",
    "# plt.savefig(f'{RESULTS_DIR}/figures/manhattan_metagenes_blitzgsea.pdf', bbox_inches='tight');\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/manhattan_metagenes_blitzgsea.pdf', bbox_inches='tight');\n",
    "# plt.savefig('overleaf/figures/manhattan_blitzgsea.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Families of pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_KEGG_human_pathways():\n",
    "    lines = REST.kegg_list('pathway', 'hsa').readlines()\n",
    "    symbols = np.array([s.split('\\t')[0].split(':')[-1] for s in lines])\n",
    "    description = np.array([s.split('\\t')[1].rstrip() for s in lines])\n",
    "    return symbols, description\n",
    "\n",
    "def get_pathway_class(pathway):\n",
    "    pathway_file = REST.kegg_get(pathway).read()  # query and read each pathway\n",
    "    \n",
    "    pathway_class = None \n",
    "    for line in pathway_file.rstrip().split('\\n'):\n",
    "        section = line[:12].strip()  # section names are within 12 columns\n",
    "        if not section == '':\n",
    "            current_section = section\n",
    "\n",
    "        if current_section == 'CLASS':\n",
    "            if pathway_class is not None:\n",
    "                print('Pathway belongs to more than one class')\n",
    "                break\n",
    "            pathway_class = line[12:]\n",
    "                \n",
    "    return pathway_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp, hp_desc = list_KEGG_human_pathways()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_significant = results_df[results_df['fdr'] < 0.05].copy()\n",
    "pathway_classes_dict = {}\n",
    "for term in tqdm(np.unique(results_df_significant.index)):\n",
    "    pathway_idx = np.where([term.lower() in p.lower() for p in hp_desc])[0]\n",
    "    if len(pathway_idx) == 0:\n",
    "        pathway_classes_dict[term] = 'Unknown'\n",
    "    else:\n",
    "        pathway_idx = pathway_idx[0]\n",
    "        pathway_code = hp[pathway_idx]\n",
    "        pathway_classes_dict[term] = get_pathway_class(pathway_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families_dict = {k: p.split(';')[0] for k, p in pathway_classes_dict.items()}\n",
    "classes_dict = {k: p.split(';')[1].lstrip() if len(p.split(';')) > 1 else p.split(';')[0] for k, p in pathway_classes_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_significant['class'] = results_df_significant.index.map(classes_dict)\n",
    "results_df_significant['Category'] = results_df_significant.index.map(families_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    indexes = np.unique(a, return_index=True)[1]\n",
    "    return a[np.sort(indexes)]\n",
    "\n",
    "sorted_idxs = np.argsort(results_df_significant['Category'].values)\n",
    "\n",
    "f(np.array(results_df_significant['class'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_classes = []\n",
    "for c in np.array(results_df_significant['class'].values)[sorted_idxs]:\n",
    "    if c not in sorted_classes:\n",
    "        sorted_classes.append(c)\n",
    "sorted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(15, 3))\n",
    "sns.countplot(x='class', # hue='Category',\n",
    "            data=results_df_significant.reset_index(),\n",
    "            # height=4,\n",
    "            # aspect=4,\n",
    "            ax=plt.gca(),\n",
    "            order=sorted_classes)\n",
    "# plt.gca().legend(loc='upper center', bbox_to_anchor=(0.5, -0.9), fancybox=True, shadow=True, ncol=4)\n",
    "plt.xticks(rotation=90);\n",
    "plt.xlabel('')\n",
    "# plt.ylabel('')\n",
    "plt.title('Number of enriched terms per type of pathway')\n",
    "# plt.savefig(f'{RESULTS_DIR}/figures/enriched_terms_pathway_type_blitzgsea.pdf', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_significant[results_df_significant['class'] == 'Neurodegenerative disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FDR neurodegenerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.05\n",
    "aggregated_df = pd.DataFrame()\n",
    "for m in range(50):\n",
    "    df = results_df[results_df['Metagene'] == m].reset_index()[['Term', 'fdr', 'Factor']].set_index(['Term', 'Factor']).unstack()\n",
    "    aggregated_df[m] = df.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurodegenerative_pathways = ['PATHWAYS OF NEURODEGENERATION',\n",
    "                              'AMYOTROPHIC LATERAL SCLEROSIS', \n",
    "                              'ALZHEIMER DISEASE',\n",
    "                              'PARKINSON DISEASE',\n",
    "                              'HUNTINGTON DISEASE',\n",
    "                              'PRION DISEASE',\n",
    "                              'SPINOCEREBELLAR ATAXIA']\n",
    "\n",
    "min_fdr_per_term = aggregated_df.min(axis=1)\n",
    "mean_fdr_per_term = aggregated_df.mean(axis=1)\n",
    "df = aggregated_df[min_fdr_per_term < 0.05]\n",
    "mask = [s in neurodegenerative_pathways for s in df.index]  \n",
    "df = df[mask]\n",
    "fdr_mask = df.values < 0.05\n",
    "df = -np.log10(df + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum([s in neurodegenerative_pathways for s in results_df[results_df['fdr'] < 0.05].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "x = np.arange(50)\n",
    "y = np.arange(df.shape[0])\n",
    "x_, y_ = np.meshgrid(x, y)\n",
    "sizes = 20*df.values  # 0.01/(df.values+1e-5)\n",
    "\n",
    "sizes = sizes[fdr_mask]\n",
    "x_ = x_[fdr_mask]\n",
    "y_ = y_[fdr_mask]\n",
    "c = df.values[fdr_mask]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "cmap = plt.cm.plasma\n",
    "norm = matplotlib.colors.Normalize()\n",
    "plt.scatter(x_.flatten(), y_.flatten(), s=sizes, c=c, norm=norm, cmap=cmap)\n",
    "plt.yticks(ticks=y, labels=df.index.values)\n",
    "cbar = plt.colorbar(fraction=0.03)\n",
    "cbar.set_label('$-\\log_{10}(q)$', rotation=270, labelpad=10)\n",
    "\n",
    "minorLocator = MultipleLocator(base=1.0)\n",
    "plt.gca().xaxis.set_minor_locator(minorLocator)\n",
    "plt.gca().grid(which='both')\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.xlim(-1, 50)\n",
    "plt.ylim(-0.5, 6.5)\n",
    "\n",
    "plt.xlabel('Metagene')\n",
    "plt.title('GSEA FDR for pathways of neurodegeneration (KEGG)')\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/fdr_scatter_metagenes_neurodegeneration_blitzgsea.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FDR signaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.05\n",
    "aggregated_df = pd.DataFrame()\n",
    "for f in range(99):\n",
    "    df = results_df[results_df['Factor'] == f].reset_index()[['Term', 'fdr', 'Metagene']].set_index(['Term', 'Metagene']).unstack()\n",
    "    aggregated_df[f] = df.min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signaling_pathways = [c for c, f in classes_dict.items() if f == 'Signaling molecules and interaction' or f == 'Signal transduction' or f == 'Membrane transport']\n",
    "\n",
    "min_fdr_per_term = aggregated_df.min(axis=1)\n",
    "mean_fdr_per_term = aggregated_df.mean(axis=1)\n",
    "df = aggregated_df[min_fdr_per_term < 0.05]\n",
    "mask = [s in signaling_pathways for s in df.index]  \n",
    "df = df[mask]\n",
    "fdr_mask = df.values < 0.05\n",
    "df = -np.log10(df + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([s in signaling_pathways for s in results_df[results_df['fdr'] < 0.05].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "x = np.arange(99)\n",
    "y = np.arange(df.shape[0])\n",
    "x_, y_ = np.meshgrid(x, y)\n",
    "sizes = 50*df.values  # 0.01/(df.values+1e-5)\n",
    "\n",
    "sizes = sizes[fdr_mask]\n",
    "x_ = x_[fdr_mask]\n",
    "y_ = y_[fdr_mask]\n",
    "c = df.values[fdr_mask]\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "cmap = plt.cm.plasma\n",
    "norm = matplotlib.colors.Normalize()\n",
    "plt.scatter(x_.flatten(), y_.flatten(), s=sizes, c=c, norm=norm, cmap=cmap)\n",
    "plt.yticks(ticks=y, labels=df.index.values)\n",
    "cbar = plt.colorbar(fraction=0.03)\n",
    "cbar.set_label('$-\\log_{10}(q)$', rotation=270, labelpad=10)\n",
    "\n",
    "minorLocator = MultipleLocator(base=1.0)\n",
    "plt.gca().xaxis.set_minor_locator(minorLocator)\n",
    "plt.gca().grid(which='both')\n",
    "plt.gca().set_axisbelow(True)\n",
    "plt.xlim(-1, 99)\n",
    "# plt.ylim(-0.5, 6.5)\n",
    "\n",
    "plt.xlabel('Metagene')\n",
    "plt.title('GSEA FDR for signaling pathways (KEGG)')\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/fdr_scatter_factors_signaling_blitzgsea.png', bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neurodegenerative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df = pd.read_csv(METADATA_FILE, delimiter='\\t')\n",
    "subject_df = subject_df.set_index('SUBJID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_val_dataset_ = HypergraphDataset(adata,\n",
    "                                  obs_source={'Tissue': ['Brain_Cortex']})\n",
    "aux_val_loader_ = DataLoader(aux_val_dataset_, batch_size=len(aux_val_dataset_),\n",
    "                             collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Compute predictions and score\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    d = next(iter(aux_val_loader_))\n",
    "    d = d.to(device)\n",
    "    x_source = d.x_source\n",
    "    x_source = model.encode_metagenes(x_source)\n",
    "\n",
    "x_source = x_source.detach().cpu().numpy()\n",
    "participant_idxs = d.source['Participant ID'].detach().cpu().numpy()\n",
    "participant_idxs = [aux_val_dataset_.donor_map[p] for p in participant_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/encoded_metagenes_brain_cortex.npy', 'wb') as f:\n",
    "    np.save(f, x_source)\n",
    "\n",
    "with open('results/participant_idxs_brain_cortex.npy', 'wb') as f:\n",
    "    np.save(f, participant_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "factor_id = 95\n",
    "metagene_idx = 11\n",
    "gene_idxs = np.argsort(metagene_w[metagene_idx, factor_id, :])[::-1]\n",
    "gene_names = adata.var['Symbol'][gene_idxs].values\n",
    "gene_values = metagene_w[metagene_idx, factor_id, gene_idxs]\n",
    "signature = pd.DataFrame({0: gene_names, 1: gene_values})\n",
    "# result = blitz.gsea(signature, library, permutations=100)\n",
    "\n",
    "df = results_df[(results_df['Factor'] == factor_id) & (results_df['Metagene'] == metagene_idx)]\n",
    "fig = blitz.plot.top_table(signature, library, df, n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = blitz.plot.running_sum(signature, df.index[0], library, result=df, compact=False)\n",
    "plt.suptitle(f'Metagene {metagene_idx}, factor {factor_id}')\n",
    "fig.set_size_inches((5, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'MHALZDMT'\n",
    "x = x_source[:, 11, :]\n",
    "\n",
    "x_2d = umap.UMAP(random_state=0).fit_transform(x)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "y = subject_df.loc[participant_idxs][key]\n",
    "plt.scatter(x_2d[y==0, 0], x_2d[y==0, 1], s=20, cmap=matplotlib.cm.summer, label='Control')\n",
    "plt.gca().scatter(x_2d[y==1, 0], x_2d[y==1, 1], s=50, marker='^', cmap=matplotlib.cm.summer, label='ALZDMT')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Alzheimer or dementia (brain cortex)')\n",
    "plt.xlabel('Metagene 11, UMAP 1')\n",
    "plt.ylabel('Metagene 11, UMAP 2')\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/metagenes_ALZDMT_cortex.pdf', bbox_inches='tight');\n",
    "Counter(y), Counter(subject_df[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_results_df.loc['AMYOTROPHIC LATERAL SCLEROSIS'].sort_values('fdr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypergraph baseline (all tissues)\n",
    "# Select same set of individuals\n",
    "aux_val_dataset_ = HypergraphDataset(adata,\n",
    "                                  obs_source={'Tissue': ['Brain_Spinal_cord']})\n",
    "aux_val_loader_ = DataLoader(aux_val_dataset_, batch_size=len(aux_val_dataset_),\n",
    "                             collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Compute predictions and score\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    d = next(iter(aux_val_loader_))\n",
    "    d = d.to(device)\n",
    "    x_source = d.x_source\n",
    "    x_source = model.encode_metagenes(x_source)\n",
    "\n",
    "x_source = x_source.detach().cpu().numpy()\n",
    "participant_idxs = d.source['Participant ID'].detach().cpu().numpy()\n",
    "participant_idxs = [aux_val_dataset_.donor_map[p] for p in participant_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/encoded_metagenes_brain_spinal_cord.npy', 'wb') as f:\n",
    "    np.save(f, x_source)\n",
    "\n",
    "with open('results/participant_idxs_brain_spinal_cord.npy', 'wb') as f:\n",
    "    np.save(f, participant_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "key = 'MHALS' # 'MHALS'\n",
    "x = x_source[:, 11, :]\n",
    "\n",
    "x_2d = umap.UMAP(random_state=0).fit_transform(x)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "y = subject_df.loc[participant_idxs][key]\n",
    "plt.scatter(x_2d[y==0, 0], x_2d[y==0, 1], s=20, cmap=matplotlib.cm.summer, label='Control')\n",
    "plt.gca().scatter(x_2d[y==1, 0], x_2d[y==1, 1], s=50, marker='^', cmap=matplotlib.cm.summer, label=key)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Amyotrophic Lateral Sclerosis (spinal cord)')\n",
    "plt.xlabel('Metagene 11, UMAP 1')\n",
    "plt.ylabel('Metagene 11, UMAP 2')\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/metagenes_MHALS_spinalcord.pdf', bbox_inches='tight');\n",
    "Counter(y), Counter(subject_df[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_id = 95\n",
    "metagene_idx = 11\n",
    "gene_idxs = np.argsort(metagene_w[metagene_idx, factor_id, :])[::-1]\n",
    "gene_names = adata.var['Symbol'][gene_idxs].values\n",
    "gene_values = metagene_w[metagene_idx, factor_id, gene_idxs]\n",
    "signature = pd.DataFrame({0: gene_names, 1: gene_values})\n",
    "# result = blitz.gsea(signature, library, permutations=100)\n",
    "\n",
    "df = results_df[(results_df['Factor'] == factor_id) & (results_df['Metagene'] == metagene_idx)]\n",
    "fig = blitz.plot.top_table(signature, library, df, n=15)\n",
    "plt.title(f'Metagene {metagene_idx}, factor {factor_id}')\n",
    "fig.set_size_inches((5, 7))\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/metagenes_ALS_ALZDMT_top_plot.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = blitz.plot.running_sum(signature, df.index[0], library, result=df, compact=False)\n",
    "plt.suptitle(f'Metagene {metagene_idx}, factor {factor_id}')\n",
    "fig.set_size_inches((5, 7))\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/metagenes_ALS_running_sum.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = blitz.plot.running_sum(signature, df.index[1], library, result=df, compact=False)\n",
    "plt.suptitle(f'Metagene {metagene_idx}, factor {factor_id}')\n",
    "fig.set_size_inches((5, 7))\n",
    "plt.savefig(f'{RESULTS_DIR}/figures/metagenes_Alzheimer_running_sum.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(participant_ids, tissue_ids, expression, donor_map, tissue_dict_inv, symbols):\n",
    "    participant_ids = np.concatenate(participant_ids, axis=0)\n",
    "    tissue_ids = np.concatenate(tissue_ids, axis=0)\n",
    "    expression = np.concatenate(expression, axis=0)\n",
    "    df_metadata = pd.DataFrame({'Participant ID': [donor_map[p] for p in participant_ids],\n",
    "                               'Tissue': [tissue_dict_inv[t] for t in tissue_ids]})\n",
    "    df = pd.DataFrame(expression, columns=symbols)\n",
    "    df = pd.concat([df_metadata, df], axis=1)\n",
    "    df = df.set_index('Participant ID')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HypergraphDataset(adata, static=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# df_imputed = pd.DataFrame({'Participant ID': [], 'Tissue ID': [], })\n",
    "source_participant_ids = []\n",
    "source_tissue_ids = []\n",
    "source_expression = []\n",
    "target_participant_ids = []\n",
    "target_tissue_ids = []\n",
    "target_expression = []\n",
    "for i, d in tqdm(enumerate(dataset)):\n",
    "    # Set target tissues to missing tissues\n",
    "    d.target['Tissue'] = torch.tensor([t for t in np.arange(len(tissue_dict)) if t not in d.source['Tissue']])\n",
    "    d.target['Participant ID'] = torch.zeros_like(d.target['Tissue']) + d.source['Participant ID'][0]\n",
    "    d.x_target = torch.tensor([-1])  # Unused\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        out, node_features = forward(d, model, device, preprocess_fn=None) \n",
    "        y_pred = out['px_rate']\n",
    "    \n",
    "    # Store\n",
    "    source_participant_ids.append(d.source['Participant ID'].cpu().numpy() + i)\n",
    "    source_tissue_ids.append(d.source['Tissue'].cpu().numpy())\n",
    "    source_expression.append(d.x_source.cpu().numpy())\n",
    "    target_participant_ids.append(d.target['Participant ID'].cpu().numpy() + i)\n",
    "    target_tissue_ids.append(d.target['Tissue'].cpu().numpy())\n",
    "    target_expression.append(y_pred.cpu().numpy())\n",
    "\n",
    "# Store data in dataframes\n",
    "df_imputed = create_dataframe(target_participant_ids, target_tissue_ids, target_expression,\n",
    "                              donor_map=dataset.donor_map,\n",
    "                              tissue_dict_inv=tissue_dict_inv,\n",
    "                              symbols=adata.var['Symbol'])\n",
    "df_observed = create_dataframe(source_participant_ids, source_tissue_ids, source_expression,\n",
    "                               donor_map=dataset.donor_map,\n",
    "                               tissue_dict_inv=tissue_dict_inv,\n",
    "                               symbols=adata.var['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_observed.to_csv(f'{RESULTS_DIR}/observed_normalised.csv')\n",
    "df_imputed.to_csv(f'{RESULTS_DIR}/imputed_normalised.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tissue to tissue network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_scores = np.load(f'{RESULTS_DIR}/t2t_scores.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fn = pearson_correlation_score\n",
    "t2t_scores_dict = {}\n",
    "unseen_mask = np.logical_or(val_mask, test_mask)\n",
    "\n",
    "for st in tissue_dict.keys():\n",
    "    st2t_scores_dict = {}\n",
    "    for tt in tqdm(tissue_dict.keys()):\n",
    "        print(st, '->', tt)\n",
    "        if st in t2t_scores_dict and tt in t2t_scores_dict[st]:\n",
    "            continue\n",
    "            \n",
    "        # Name source and target tissues\n",
    "        source_name = st.replace('_', ' ')\n",
    "        target_name = tt.replace('_', ' ')\n",
    "        # print(tt)\n",
    "\n",
    "        # Create datasets\n",
    "        aux_dataset = HypergraphDataset(adata[unseen_mask],\n",
    "                                        obs_source={'Tissue': [st]},\n",
    "                                        obs_target={'Tissue': [tt]})\n",
    "        source_donor_ids = aux_dataset.adata_source.obs['Participant ID']\n",
    "        target_donor_ids = aux_dataset.adata_target.obs['Participant ID']\n",
    "        assert (source_donor_ids.values == target_donor_ids.values).all()\n",
    "        \n",
    "        if len(aux_dataset) < 10:\n",
    "            print('Less than 10 samples', st, tt)\n",
    "            continue\n",
    "\n",
    "        # Hypergraph baseline\n",
    "        aux_loader = DataLoader(aux_dataset, batch_size=len(aux_dataset),\n",
    "                                collate_fn=collate_fn, shuffle=False, drop_last=False)\n",
    "\n",
    "        # Compute predictions and score\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            d = next(iter(aux_loader))\n",
    "\n",
    "            out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "            y_test_pred = out['px_rate'].cpu().numpy()  # torch.distributions.normal.Normal(loc=out['px_rate'], scale=out['px_r']).mean.cpu().numpy()\n",
    "            y_test_ = d.x_target.cpu().numpy()\n",
    "\n",
    "        sample_scores = score_fn(y_test_, y_test_pred, sample_corr=True)\n",
    "        gene_scores = score_fn(y_test_, y_test_pred, sample_corr=False)\n",
    "\n",
    "        # Append results\n",
    "        st2t_scores_dict[tt] = {'gene_scores': gene_scores.mean(),\n",
    "                                'sample_scores': sample_scores.mean()}\n",
    "        del aux_dataset\n",
    "        del aux_loader\n",
    "        \n",
    "    t2t_scores_dict[st] = st2t_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2t_scores = np.zeros((len(tissue_dict), len(tissue_dict)))\n",
    "for i, st in enumerate(tissue_dict.keys()):\n",
    "    for j, tt in enumerate(tissue_dict.keys()):\n",
    "        if tt in t2t_scores_dict[st]:\n",
    "            t2t_scores[i, j] = t2t_scores_dict[st][tt]['gene_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#ffaa56', '#cdad22', '#8fbc8f', '#8b1c62', '#ee6a50', '#ff0000', '#eeee00', '#eeee00', '#eeee00',\n",
    "          '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00', '#eeee00',\n",
    "          '#eeee00', '#00cdcd', '#9ac0cd', '#ee82ee', '#cdb79e', '#eec591', '#8b7355', '#8b7355', '#cdaa7d',\n",
    "          '#b452cd', '#7a378b', '#cdb79e', '#cdb79e', '#9acd32', '#cdb79e', '#7A67EE', '#FFD700', '#FFB6C1',\n",
    "          '#CD9B1D', '#B4EEB4', '#D9D9D9', '#3A5FCD', '#1E90FF', '#CDB79E', '#CDB79E', '#FFD39B', '#A6A6A6',\n",
    "          '#008B45', '#EED5D2', '#EED5D2', '#FF00FF']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "threshold = 0.4\n",
    "t2t_scores_ = t2t_scores * (1 - np.eye(t2t_scores.shape[0]))\n",
    "G = nx.from_numpy_matrix(t2t_scores_ > threshold, create_using=nx.DiGraph)\n",
    "G = nx.relabel_nodes(G, tissue_dict_inv)\n",
    "G.remove_nodes_from(list(nx.isolates(G)))\n",
    "pos = nx.circular_layout(G)  # \n",
    "# pos = nx.spring_layout(G, pos=pos, k=0.1, iterations=2)\n",
    "\n",
    "edge_weights = np.array([t2t_scores[tissue_dict[u], tissue_dict[v]] for u,v in G.edges])\n",
    "# edge_weights = 0.2 + 2*(edge_weights - np.min(edge_weights))/ (np.max(edge_weights) - np.min(edge_weights))\n",
    "node_size = np.array([G.degree[u]*10 for u in G.nodes])\n",
    "labels = {k: k.replace('_', ' ').replace('Brain ', '').replace(' Omentum', '').replace(' Tissue', '') for k in G.nodes.keys()}\n",
    "\n",
    "color_map = [colors[tissue_dict[t]] for t in G.nodes]\n",
    "\n",
    "# nx.draw(G, pos=pos, with_labels = True, width=weights)  # node_size=[d[k]*100 for k in d]\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.3, width=edge_weights, edge_color=\"gray\")\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color=color_map, alpha=0.9)\n",
    "\n",
    "labels_pos = {}\n",
    "n = len(pos)\n",
    "shiftval = 0.12\n",
    "for i, (k, v) in enumerate(pos.items()):\n",
    "    labels_pos[k] = pos[k] + np.sqrt(len(labels[k]))*np.array([shiftval * np.cos(i*2*np.pi/n), shiftval * np.sin(i*2*np.pi/n)])\n",
    "    \n",
    "    if k == 'Esophagus_Muscularis':\n",
    "        labels_pos[k] += (-0.07, -0.01)\n",
    "    elif k == 'Esophagus_Mucosa':\n",
    "        labels_pos[k] += (-0.06, -0.0)\n",
    "    elif k == 'Esophagus_Gastro':\n",
    "        labels_pos[k] += (-0.06, +0.01)\n",
    "    elif k == 'Colon_Transverse':\n",
    "        labels_pos[k] += (-0.05, +0.02)\n",
    "    elif k == 'Colon_Sigmoid':\n",
    "        labels_pos[k] += (-0.04, +0.03)\n",
    "    elif k == 'Breast_Mammary_Tissue':\n",
    "        labels_pos[k] += (-0.03, +0.04)\n",
    "\n",
    "\n",
    "text = nx.draw_networkx_labels(G, labels_pos, labels, font_size=12);\n",
    "for i,(_,t) in enumerate(text.items()):\n",
    "    angle = 360*i/len(text.items())\n",
    "    if np.cos(angle*(np.pi/180)) < 0:\n",
    "        angle = angle+180\n",
    "    t.set_rotation(angle)\n",
    "\n",
    "plt.gca().axis('off')\n",
    "\n",
    "marginval = 0.9\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((x1-marginval,x2+marginval,y1-marginval,y2+marginval))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/tissue_to_tissue_network_{threshold}cutoff_pergene.pdf', bbox_inches='tight');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitissue",
   "language": "python",
   "name": "multitissue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
