{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa812808",
   "metadata": {},
   "source": [
    "# Hypergraph factorisation tutorial\n",
    "\n",
    "In this notebook we present some of the main features of the Pytorch implementation of the Hypergraph Factorisation method, including the flexible hypergraph dataset (the implementation supports k-uniform hypergraphs for any k) and HYFA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16047fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747e692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rv340/anaconda3/envs/multitissue/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from train_gtex import *\n",
    "from src.train_utils import forward\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d6f99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'data/normalised_model_default.pth'\n",
    "GTEX_FILE = 'data/GTEX_data.csv'\n",
    "METADATA_FILE = 'data/GTEx_Analysis_v8_Annotations_SubjectPhenotypesDS.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d69f9",
   "metadata": {},
   "source": [
    "---\n",
    "## Load GTEx data\n",
    "\n",
    "Let's load the normalised GTEx data in [AnnData](https://anndata.readthedocs.io/en/latest/) format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299e59d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/homes/rv340/HYFA/train_gtex.py:69: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs['Tissue_idx'], tissue_dict = map_to_ids(adata.obs['Tissue'].values)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 15197 × 12557\n",
       "    obs: 'Participant ID', 'Tissue', 'Tissue_idx', 'Participant ID_dyn', 'Age', 'Sex'\n",
       "    var: 'Symbol'\n",
       "    uns: 'Tissue_dict', 'Sex_dict', 'Tissue_colors'\n",
       "    obsm: 'Participant ID_feat'\n",
       "    layers: 'x'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "adata = GTEx_v8_normalised_adata(file=GTEX_FILE)\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975724ae",
   "metadata": {},
   "source": [
    "Here is a brief description of the `adata` fields:\n",
    "* **obs**\n",
    "    * `adata.obs['Participant ID']`: GTEx individual of every sample.\n",
    "    * `adata.obs['Tissue']`: GTEx tissue of every sample.\n",
    "    * `adata.obs['Tissue_idx']`: Encoded GTEx tissue. HYFA treats all fields in obs ending with `_idx` as static node types, i.e. node types with learnable node features.\n",
    "    * `adata.obs['Participant ID_dyn']`: Encoded GTEx donor. HYFA treats all fields in obs ending with `_dyn` as dynamic node types, i.e. node types that are updated via message passing.\n",
    "\n",
    "* **var**: Gene symbols\n",
    "\n",
    "* **uns**:\n",
    "    * `adata.uns['Tissue_dict']`: Dictionary mapping GTEx tissues names to identifiers.\n",
    "    * `adata.uns['Sex_dict']`: Dictionary mapping sex names to identifiers.\n",
    "\n",
    "* **obsm**\n",
    "    * `adata.obsm['Participant ID_feat']`: For every field `adata.obs['*_dyn']`, `adata.obsm['*_dyn']` contains the initial node features. For example, in this case the initial node features of the individual nodes are the age and sex of the individuals.\n",
    "\n",
    "* **layers**:\n",
    "    * `adata.layers['x']` contrains the processed gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03f853",
   "metadata": {},
   "source": [
    "---\n",
    "## Hypergraph dataset\n",
    "\n",
    "We split the data by donors into train, validation, and test datasets using our predefined donor splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc87ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/val/test\n",
    "donors = adata.obs['Participant ID'].values\n",
    "train_donors = np.loadtxt('data/splits/gtex_train.txt', delimiter=',', dtype=str)\n",
    "val_donors = np.loadtxt('data/splits/gtex_val.txt', delimiter=',', dtype=str)\n",
    "test_donors = np.loadtxt('data/splits/gtex_test.txt', delimiter=',', dtype=str)\n",
    "train_mask = np.isin(donors, train_donors)\n",
    "test_mask = np.isin(donors, test_donors)\n",
    "val_mask = np.isin(donors, val_donors)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = HypergraphDataset(adata[train_mask],\n",
    "                                  disjoint=True, static=False)\n",
    "val_dataset = HypergraphDataset(adata[val_mask],\n",
    "                                disjoint=True, static=True)\n",
    "test_dataset = HypergraphDataset(adata[test_mask],\n",
    "                                 static=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079cd3a",
   "metadata": {},
   "source": [
    "`HypergraphDataset` is a Pytorch dataset that takes care of obtaining samples by individual (i.e. argument `donor_key`). This flexible dataset has several arguments that control its behaviour:\n",
    "* `adata`: AnnData object with the gene expression data\n",
    "* `adata_target` (optional): Another AnnData object (same format as `adata`) with the target observations. The dataset matches individuals by `donor_key` (and discards unmatched individuals).\n",
    "* `obs_source` (optional): Dictionary: AnnData `obs` keys -> selected values for the given `obs` key. The dataset filters samples from `adata` according to this dictionary. For example, the following `obs_source`\n",
    "```\n",
    "obs_source={'Tissue': ['Whole_Blood', 'Adipose_Visceral_Omentum'],\n",
    "            'Participant ID': ['GTEX-1192X']}\n",
    "```\n",
    "will select the whole blood and adipose visveral omentum gene expression samples of individual `GTEX-1192X`.\n",
    "* `obs_target` (optional): same as `obs_source`, but appied to the target data `adata_target`.\n",
    "* `static`: Boolean value indicating whether to use static or dynamic samples. If `static=False` and `disjoint=True`, the dataset will use dynamic inputs and outputs (e.g. useful to train the model via self-supervised learning) --- the argument `sample_fn` determines the sampling behaviour. For example, `train_dataset[0].source` will yield different source tissues in different runs. If `static=True`, the dataset doesn't do any sampling.\n",
    "* `disjoint`: When the dataset is not static, this argument allows to draw mutually exclusive samples via the `sample_fn` argument. By default, this samples two random sets of \"collected\" and \"uncollected\" tissues for every individual and enforces that the two sets are mutually exclusive (i.e. no overlap between source and target samples).\n",
    "---\n",
    "\n",
    "Let's now examine the contents of a dataset sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c65a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c40ef",
   "metadata": {},
   "source": [
    "The dataset returns `Data` instances with several fields, including `source`, `target`, `x_source`, `x_target`, `source_dynamic`, `target_dynamic`, `source_features`, and `target_features` among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e92936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tissue': tensor([ 4, 46]), 'Participant ID': tensor([0, 0])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211594c",
   "metadata": {},
   "source": [
    "The `source` field contains the `obs` (mapped) information of the source donors (see section above with fields of AnnData object), e.g. IDs of both dynamic and static node types. You can map the static nodes (e.g. tissues) back to the original names through the dictionary that was used to map the static node names to identifiers. For example, for the GTEx tissues, we stored this dictionary in `adata.uns['Tissue_dict']`. Similarly, `d.target` contains the information for the target samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa948d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tissue': tensor([ 0,  1,  5, 11, 19, 27, 32, 33, 39, 47]),\n",
       " 'Participant ID': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b032a59b",
   "metadata": {},
   "source": [
    "The unmapped information of the dynamic nodes (e.g. `Participant ID`) is available through `d.source_dynamic` (source samples) and `d.target_dynamic` (target samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f5bda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Participant ID': 1670     GTEX-1117F\n",
       " 14261    GTEX-1117F\n",
       " Name: Participant ID_dyn, dtype: object}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.source_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3569843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Participant ID': 0        GTEX-1117F\n",
       " 581      GTEX-1117F\n",
       " 1883     GTEX-1117F\n",
       " 3321     GTEX-1117F\n",
       " 4648     GTEX-1117F\n",
       " 7652     GTEX-1117F\n",
       " 9206     GTEX-1117F\n",
       " 9350     GTEX-1117F\n",
       " 11518    GTEX-1117F\n",
       " 14390    GTEX-1117F\n",
       " Name: Participant ID_dyn, dtype: object}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.target_dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97ccfa",
   "metadata": {},
   "source": [
    "The initial node features of the source and target nodes are available via `d.source_features` and `d.target_features`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78d3c564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Participant ID': tensor([[0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000],\n",
       "         [0.6000, 1.0000]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.target_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc497419",
   "metadata": {},
   "source": [
    "Note that we used the arguments `disjoint=True` and `static=False` for the train dataset, so the dataset is dynamically drawing samples that are mutually exclusive, i.e. non-overlapping sets of tissues. The corresponding gene expression information of the source and target samples is stored in `d.x_source` and `d.x_target`, correspondingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aba3560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5406,  0.1529,  0.0469,  ..., -0.6967, -0.5406,  0.5271],\n",
       "        [ 1.1228, -0.4806, -0.1940,  ..., -0.0193, -0.2137, -0.9272]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c73a05f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7381, -0.1775, -2.5653,  ...,  2.5653,  2.5653, -2.2045],\n",
       "        [ 1.3853, -0.9533,  2.3030,  ...,  0.0640,  0.3369, -0.6711],\n",
       "        [ 0.7879, -1.0493,  0.7533,  ..., -0.8055,  2.7047, -1.2206],\n",
       "        ...,\n",
       "        [ 0.2857,  0.2673,  0.8954,  ...,  0.4766,  0.8186, -1.3120],\n",
       "        [ 1.8409, -0.5266,  0.7719,  ...,  2.0289, -1.2161,  0.1213],\n",
       "        [-0.7779,  1.5863, -0.6856,  ..., -1.9084,  0.5366,  1.8092]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff6922",
   "metadata": {},
   "source": [
    "We can map the information of a `Data` object to a Pytorch device via the `d.to(device)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2cedc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tissue': tensor([ 4, 46], device='cuda:0'),\n",
       " 'Participant ID': tensor([0, 0], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = d.to('cuda:0')\n",
    "d.source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6b32340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5406,  0.1529,  0.0469,  ..., -0.6967, -0.5406,  0.5271],\n",
       "        [ 1.1228, -0.4806, -0.1940,  ..., -0.0193, -0.2137, -0.9272]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "857e954a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7381, -0.1775, -2.5653,  ...,  2.5653,  2.5653, -2.2045],\n",
       "        [ 1.3853, -0.9533,  2.3030,  ...,  0.0640,  0.3369, -0.6711],\n",
       "        [ 0.7879, -1.0493,  0.7533,  ..., -0.8055,  2.7047, -1.2206],\n",
       "        ...,\n",
       "        [ 0.2857,  0.2673,  0.8954,  ...,  0.4766,  0.8186, -1.3120],\n",
       "        [ 1.8409, -0.5266,  0.7719,  ...,  2.0289, -1.2161,  0.1213],\n",
       "        [-0.7779,  1.5863, -0.6856,  ..., -1.9084,  0.5366,  1.8092]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76658a47",
   "metadata": {},
   "source": [
    "Now that we've covered the basics of `HypergraphDataset`, let's see how to instantiate HYFA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a35c0",
   "metadata": {},
   "source": [
    "---\n",
    "## HYFA model\n",
    "\n",
    "We start by initialising weights and biases and loading the YAML config file with HYFA's hyperparameters for the normalised GTEx data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "421caa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu': 0, 'sweep': False, 'lr': 0.00045568167228053424, 'batch_size': 63, 'epochs': 200, 'patience': 30, 'dropout': 0.17385777664292695, 'bn': True, 'use_demographic_information': True, 'update_edge_attr': False, 'n_graph_layers': 2, 'n_hidden_layers': 1, 'n_hidden_layers_pred': 2, 'n_hidden_layers_prior_var': 0, 'meta_G': 50, 'd_patient': 71, 'd_gene': 48, 'd_edge_attr': 98, 'd_tissue': 120, 'd_edge': 28, 'n_heads': 28, 'beta': 0, 'n_top_genes': None, 'attention_strategy': 'patient', 'metagenes_encoder': 'plain', 'loss_type': 'normal', 'layer': 'gat', 'activation': 'swish', 'norm': 'batch'}\n"
     ]
    }
   ],
   "source": [
    "# Initialise wandb\n",
    "wandb.init(project='multitissue_imputation', config='configs/default.yaml', mode='disabled')\n",
    "config = wandb.config\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822453e3",
   "metadata": {},
   "source": [
    "Most hyperparameters are self-explanatory. Here is a brief description of the most important:\n",
    "* `n_graph_layers`: Number of message passing layers.\n",
    "* `meta_G`: Number of metagenes. Each metagene captures one mode of interaction (e.g. similar to channels in a convolutional neural network).\n",
    "* `d_patient`: Dimensionality of the donor node features.\n",
    "* `d_gene`: Dimensionality of the metagene node features.\n",
    "* `d_tissue`: Dimensionality of the tissue node features.\n",
    "* `d_edge_attr`: Dimensionality of the hyperedge attributes.\n",
    "* `d_edge`: Dimensionality of the message vectors.\n",
    "* `n_heads`: Number of heads in the message aggregation mechanism.\n",
    "* `loss_type`: Likelihood function name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb79be3",
   "metadata": {},
   "source": [
    "We now create Pytorch data loaders for the train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbec3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data loaders\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config.batch_size,\n",
    "                          collate_fn=Data.from_datalist,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        collate_fn=Data.from_datalist,\n",
    "                        shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=config.batch_size,\n",
    "                         collate_fn=Data.from_datalist,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fd78a",
   "metadata": {},
   "source": [
    "The argument `collate_fn` specifies how to create batches from multiple data objects. The function `Data.from_datalist` takes care of merging several `Data` objects output by the dataset into a single one (i.e. it reindexes the IDs of dynamic nodes).\n",
    "\n",
    "Let's now instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c7b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use certain GPU\n",
    "device = torch.device(\"cuda:{}\".format(config.gpu) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Select dynamic/static node types\n",
    "config.update({'static_node_types': {\n",
    "    'Tissue': (len(adata.obs['Tissue_idx'].unique()), config.d_tissue),\n",
    "    'metagenes': (config.meta_G, config.d_gene)\n",
    "}}, allow_val_change=True)\n",
    "config.update({'dynamic_node_types': {\n",
    "    'Participant ID': (len(adata.obs['Participant ID'].unique()), config.d_patient)\n",
    "}}, allow_val_change=True)\n",
    "\n",
    "# Model\n",
    "config.G = adata.shape[-1]\n",
    "model = HypergraphNeuralNet(config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3876e4",
   "metadata": {},
   "source": [
    "The attributes `'static_node_types'` and `'dynamic_node_types'` of the `config` object specify the properties of the static and dynamic node types of the hypergraph. The model relies on these two fields to determine the number of node types, their dimensions, and whether they should be updated during message passing (static vs dynamic node types). For every node type (i.e. key in dictionary), we specified the number of nodes as well as their dimensions (`n_nodes`, `node_dim`). The `metagenes` field is special (the model will *not* look for this key in `d.source`) and expected by the model - it determines the number and dimension of the metagenes.\n",
    "\n",
    "We have now instantiated HYFA. To train the model, we provide a `train` function that trains the model and takes in the config object, the HYFA model, the train and validation loaders, as well as other optional keyword arguments. See the `train_gtex.py` script for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aeea5a",
   "metadata": {},
   "source": [
    "---\n",
    "## Inferring uncollected gene expression\n",
    "\n",
    "We now load the parameters of a trained HYFA model and generate predictions for uncollected tissues. This step assumes that the architecture and hyperparameters are the same used for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5d5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model parameters\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164ad29",
   "metadata": {},
   "source": [
    "Let's infer the expression of uncollected tissues for individuals in the validation set, using data from accessible tissues. First, we'll select validation individuals with gene expression collected in all GTEx accessible tissues (whole blood, skin sun exposed, skin not sun exposed, and adipose subcutaneous), as well as a target tissue, say esophagus mucosa, which we assume to be uncollected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e00ee8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_mask = val_mask\n",
    "source_tissues = ['Whole_Blood', 'Skin_Sun_Epsd', 'Skin_Not_Sun_Epsd', 'Adipose_Subcutaneous']\n",
    "target_tissues = ['Esophagus_Mucosa']\n",
    "\n",
    "valid_donors = []\n",
    "donors = adata[split_mask].obs['Participant ID'].unique()\n",
    "for donor in donors:\n",
    "    donor_mask = adata[split_mask].obs['Participant ID'] == donor\n",
    "    all_tissues_collected = all([t in adata[split_mask].obs[donor_mask]['Tissue'].values for t in source_tissues + target_tissues])\n",
    "    if all_tissues_collected:\n",
    "        valid_donors.append(donor)\n",
    "len(valid_donors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd89885",
   "metadata": {},
   "source": [
    "We now instantiate an auxiliary dataset and data loader for the selected donors in `valid_donors`. We can use `obs_source` and `obs_target` to select the donors and tissues from above. For now, we will use a single reference tissue (whole blood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1874f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 42 source and 42 target samples of 42 unique donors\n"
     ]
    }
   ],
   "source": [
    "source_tissues_1 = ['Whole_Blood']\n",
    "aux_val_dataset = HypergraphDataset(adata[split_mask],\n",
    "                                    obs_source={'Tissue': source_tissues_1, 'Participant ID': valid_donors},\n",
    "                                    obs_target={'Tissue': target_tissues, 'Participant ID': valid_donors},\n",
    "                                    static=True, \n",
    "                                    verbose=True)\n",
    "aux_val_loader = DataLoader(aux_val_dataset,\n",
    "                            batch_size=len(aux_val_dataset),\n",
    "                            collate_fn=Data.from_datalist,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f496816",
   "metadata": {},
   "source": [
    "We feed the data to HYFA to get the inferred esophagus mucosa gene expession for the selected validation individuals. To do so, we use the `forward` method which executes HYFA's workflow (infer low-dimensional metagene features for source tissues, perform message passing in the 3-uniform hypergraph, and hyperedge-level prediction to recover the uncollected gene expression). We set the argument `preprocess_fn` to `None` to indicate that the source data doesn't need to be processed. In the case of unnormalised gene expression, we can use an appropriate function (e.g. `torch.log1p`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99d7de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(aux_val_loader))\n",
    "with torch.no_grad():\n",
    "    out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "    x_pred = out['px_rate'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e7375",
   "metadata": {},
   "source": [
    "By default, if the model was trained using a normal likelihood, the attribute `'px_rate'` contains the prediction means. We'll use this as the inferred gene expression. We can now assess prediction performance by comparing the predictions with the ground truth, stored in `d.x_target`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49cd02fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39947408, 0.16053617)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFuCAYAAAC/a8I8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAd60lEQVR4nO3de3BU9f3G8ScQEolcFJRQhZ8KbXEFhVpAIkhLuIdAuAURsYgiWkcCRVFBsfUGXpBR7IggBQsyWrkINSh1QkSsBLyhomwVMSChXModsmxCku/vD4ZIBMIa2fM5gfdrxpndb86e75Oz7JPj2T17YpxzTgAAz1WxDgAAZysKGACMUMAAYIQCBgAjFDAAGKkUBbx+/fqIltu4cWN0g/wMfs3m11wS2SrCr7kk/2azzFUpCrioqCii5Q4dOhTlJBXn12x+zSWRrSL8mkvybzbLXJWigAHgTEQBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMRK2Ax44dq6SkJKWmppYZnzNnjrp166YePXroqaeeitb0AOB7sdFacd++fTV48GDdd999pWOrVq3SsmXL9M9//lNxcXHatWtXtKYHAN+L2h5wq1atVLt27TJjr776qoYPH664uDhJUt26daM1PQD4Xkw0r4qcl5enO+64Q5mZmZKktLQ0dezYUe+//77i4+N177336qqrrjrlej777DPFx8efcrlwOKxzzjnnZ+eOBr9m82suqfJmu+Syxko4J87jRJELhQu1KXeD5/P69fn0IlcgEDjheNQOQZxIcXGx9u3bp9dff11r167VqFGjtGzZMsXExJT7uPj4+JP+AscKBoMRLWfBr9n8mkuq3NkGTs/xMM0PQqGQEhISyl3mteFJJtvVr8+nZS5PPwWRmJiozp07KyYmRldddZWqVKmiPXv2eBkBAHzD0wLu1KmTVq9eLUnKzc3V4cOHdf7553sZAQB8I2qHIEaPHq0PP/xQe/bsUfv27TVixAj169dP48aNU2pqqqpVq6YnnnjilIcfAOBMFbUCnjx58gnHJ02aFK0pAaBS4Uw4ADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACNRK+CxY8cqKSlJqampx/1s5syZatKkiXbv3h2t6QHA96JWwH379tWMGTOOG9+6das++OADXXTRRdGaGgAqhagVcKtWrVS7du3jxidOnKgxY8YoJiYmWlMDQKUQ6+VkWVlZqlevni6//PKf9LiCggIFg8FTLhcOhyNazoJfs/k1l1R5swUCAYVCIY8THVFSUhLR3Bbb1a/Ppxe5AoHACcc9K+BDhw5p2rRpmjlz5k9+bHx8/El/gWMFg8GIlrPg12x+zSVV7mwJCQkepvlBKBSKaG6L7erX59Myl2efgvj++++Vl5entLQ0JScna9u2berbt6/+97//eRUBAHzFsz3gJk2aKCcnp/R+cnKy5s+frzp16ngVAQB8JWp7wKNHj9bAgQOVm5ur9u3ba968edGaCgAqpajtAU+ePLncn2dnZ0dragCoFDgTDgCMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcBIbLRWPHbsWC1fvlx169ZVZmamJOnJJ5/Uu+++q2rVqun//u//NHHiRNWqVStaEQDA16K2B9y3b1/NmDGjzFjbtm2VmZmpN998U5deeqmmTZsWrekBwPeiVsCtWrVS7dq1y4y1a9dOsbFHdrpbtGihbdu2RWt6APC9qB2COJUFCxaoe/fuES1bUFCgYDB4yuXC4XBEy1nwaza/5pIqb7ZAIKBQKORxoiNKSkoimttiu/r1+fQiVyAQOOG4SQFPnTpVVatWVa9evSJaPj4+/qS/wLGCwWBEy1nwaza/5pIqd7aEhAQP0/wgFApFNLfFdvXr82mZy/MCXrhwoZYvX66XX35ZMTExXk8PAL7haQGvWLFCM2bM0CuvvKLq1at7OTUA+E7UCnj06NH68MMPtWfPHrVv314jRozQ9OnTVVhYqKFDh0qSmjdvrkceeSRaEQDA16JWwJMnTz5uLD09PVrTAUClw5lwAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEaiVsBjx45VUlKSUlNTS8f27t2roUOHqkuXLho6dKj27dsXrekBwPeiVsB9+/bVjBkzyoxNnz5dSUlJeuedd5SUlKTp06dHa3oA8L2oFXCrVq1Uu3btMmPLli1T7969JUm9e/dWVlZWtKYHAN+L9XKyXbt2qV69epKkCy+8ULt27YrocQUFBQoGg6dcLhwOR7ScBb9m82suqfJmCwQCCoVCHic6oqSkJKK5LbarX59PL3IFAoETjntawMeKiYlRTExMRMvGx8ef9Bc4VjAYjGg5C37N5tdcUuXOlpCQ4GGaH4RCoYjmttiufn0+LXN5+imIunXraseOHZKkHTt2qE6dOl5ODwC+4mkBJycna9GiRZKkRYsWqWPHjl5ODwC+ErUCHj16tAYOHKjc3Fy1b99e8+bN0/Dhw/XBBx+oS5cuWrlypYYPHx6t6QHA96J2DHjy5MknHP/73/8erSkBoFLhTDgAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjERXwJ598EtEYACByERXwY489FtEYACBy5X4f8Jo1a7RmzRrt3r1bs2bNKh0/ePCgiouLox4OAM5k5Rbw4cOHFQqFVFxcrPz8/NLxGjVqaMqUKVEPBwBnsnILuHXr1mrdurX69Omjiy++2KtMAHBWiOiSRIWFhRo/fry2bNmioqKi0vHZs2dHLRgAnOkiKuCRI0dq4MCBSk9PV5UqfHINAE6HiAo4NjZWgwYNinYWADirRLQ726FDB82dO1c7duzQ3r17S/8DAFRcRHvAb7zxhiTpb3/7W+lYTEyMli1bFp1UAHAWiKiAs7Ozo50DAM46ERXwokWLTjjeu3fv0xgFAM4uERXw2rVrS28XFBQoJydHTZs2pYAB4GeIqIDHjx9f5v7+/fv1pz/9KSqBAOBsUaEP9VavXl15eXmnOwsAnFUi2gO+4447Sm+XlJRow4YN6t69e9RCAcDZIKICvuWWW0pvV61aVRdffLHq168ftVAAcDaI6BBE69at1ahRI+Xn52v//v2qVq3az5r05ZdfVo8ePZSamqrRo0eroKDgZ60PACqjiAr4rbfeUnp6upYuXaq333679HZFbN++XbNnz9aCBQuUmZmp4uJiLVmypELrAoDKLKJDEC+++KLmz5+vunXrSpJ2796tm2++Wd26davQpMXFxQqHw4qNjVU4HFa9evUqtB4AqMwiKmDnXGn5StJ5550n51yFJkxMTNQtt9yiDh06KD4+Xm3btlW7du3KfUxBQYGCweAp1x0OhyNazoJfs/k1l1R5swUCAYVCIY8THVFSUhLR3Bbb1a/Ppxe5AoHACccjKuB27drp1ltvVY8ePSQdOSTRvn37CgXZt2+fli1bpmXLlqlmzZoaOXKkFi9erLS0tJM+Jj4+/qS/wLGCwWBEy1nwaza/5pIqd7aEhAQP0/wgFApFNLfFdvXr82mZq9wC3rRpk3bu3Kn77rtP77zzTumVkFu0aKFevXpVaMKVK1eqQYMGqlOnjiSpS5cuWrNmTbkFDABnonLfhJswYYJq1Kgh6UhRjh07VmPHjlXnzp01YcKECk140UUX6fPPP9ehQ4fknFNOTo4aN25coXUBQGVW7h7wzp071aRJk+PGmzRpoi1btlRowubNm6tr167q06ePYmNjFQgEdP3111doXQBQmZVbwAcOHDjpz8LhcIUnzcjIUEZGRoUfDwBngnIPQTRr1kyvv/76cePz5s1T06ZNoxYKAM4G5e4Bjxs3TnfddZfefPPN0sL98ssvdfjwYf31r3/1JCAAnKnKLeALLrhAr732mlatWqX169dLkn73u98pKSnJk3AAcCaL6HPAbdq0UZs2baKdBQDOKhX6PmAAwM9HAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBo9IpLCqxjuDLKzug8onoVGTAT+Jiq2jg9BzTDOVd+ue14XxXCiLDHjAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4ARChgAjJgU8P79+5WRkaFu3bqpe/fuWrNmjUUMADBl8n3Ajz/+uK677jpNmTJFhYWFCofDFjEAwJTne8AHDhzQRx99pP79+0uS4uLiVKtWLa9jAIA5z/eA8/LyVKdOHY0dO1b/+c9/1LRpUz3wwAMnvbqAJBUUFCgYDJ5y3eFwOKLlLPg1m19zSSfPFggEFAqFDBL9oKSkpNwMVvlOlUs6ckmnuFjvjz5GchmnULhQm3I3eJDmB168Bk72u3tewEVFRVq3bp3Gjx+v5s2b67HHHtP06dM1atSokz4mPj4+oicvGAz69lpdfs3m11xS+dnK+4PthfIuSSTZ5TtVLsnukk6RZHtteJLn/x4tXwOe/xmsX7++6tevr+bNm0uSunXrpnXr1nkdAwDMeV7AF154oerXr6/vvvtOkpSTk6PGjRt7HQMAzJl8CmL8+PG65557dPjwYTVs2FATJ060iAEApkwKOBAIaOHChRZTA4BvcCYcABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQrYSGFRiXUESSe/VIpf8gFnMpOvo4TdZWF+7GSXiXlteJJBGuDswh4wABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIyYFXBxcbF69+6t22+/3SoCAJgyK+DZs2ercePGVtMDgDmTAt62bZuWL1+u/v37W0wPAL5gckWMCRMmaMyYMcrPz49o+YKCAgWDwVMuFw6HI1rOwo+zBQIBhUIhw0RHlJSUnDBHYVGJ4mJt3yI42eWSJJlvu5Ntt6Os8p0q11EW+SLN5vVr2IveONm/Zc8L+N1331WdOnXUrFkzrV69OqLHxMfHl/tiPCoYDEa0nIUTZTvRpYC8drJLEvnhkknlXS7JetudLNtRVvlOlesoi3yRZvP6NWzZG54X8Keffqrs7GytWLFCBQUFOnjwoO655x5NmjTJ6ygAYMrzAr777rt19913S5JWr16tmTNnUr4Azkp8DhgAjJhelv6aa67RNddcYxkBAMywBwwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABgJNbrCbdu3ap7771Xu3btUkxMjAYMGKAhQ4Z4HQMAzHlewFWrVtX999+vpk2b6uDBg+rXr5/atm2rX/7yl15HAQBTnh+CqFevnpo2bSpJqlGjhho1aqTt27d7HQMAzJkeA87Ly1MwGFTz5s0tYwCACc8PQRyVn5+vjIwMjRs3TjVq1Ch32YKCAgWDwVOuMxwOl1nukssaK+GcuJ+d9XQIBALHjYVCIYMkZZWUlJw0h3W+yppNsst3qlxHWeSLJFthUYniYr3dLzzRa/NkQuFCbcrdcNrmMCngw4cPKyMjQz179lSXLl1OuXx8fHxEGykYDB633MDpORXOeTqFQiElJCSU3n9teFKZ+1Z+nOtY1vkqazbJLt+pch1lkS+SbHGxVTx/zUa6zaQjr9ufUtin4vkhCOecHnjgATVq1EhDhw71enoA8A3PC/iTTz7R4sWLtWrVKqWlpSktLU3vvfee1zEAwJznhyBatmypr7/+2utpAcB3OBMOAIxQwABghAIGACMUMAAYoYABwAgFDABGKGAAMEIBA4ARChgAjFDAAGCEAgYAIxQwABihgAHACAUMAEYoYAAwQgEDgBEKGACMUMAAYIQCBgAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIxQwABghAIGACMUMAAYoYABwAgFDABGTAp4xYoV6tq1qzp37qzp06dbRAAAc54XcHFxsR555BHNmDFDS5YsUWZmpr799luvYwCAOc8L+IsvvtAll1yihg0bKi4uTj169NCyZcu8jgEA5mKcc87LCZcuXar3339fjz/+uCRp0aJF+uKLL/TQQw+d9DGfffaZ4uPjvYoIAKdVbGysfvWrXx0/bpDlJ2vRooV1BAA47Tw/BJGYmKht27aV3t++fbsSExO9jgEA5jwv4CuvvFIbN27U5s2bVVhYqCVLlig5OdnrGABgzvNDELGxsXrooYc0bNgwFRcXq1+/fic8NgIAZzrP34QDABzBmXAAYIQCBgAjlbKAT3Uqc2FhoUaNGqXOnTsrPT1deXl5vsn20UcfqU+fPrriiiu0dOlS3+SaNWuWUlJS1LNnTw0ZMkRbtmzxTbZXX31VPXv2VFpamm644QbPzpyM9JT5f/3rX2rSpInWrl3rSa5Isi1cuFBt2rRRWlqa0tLSNG/ePF/kkqS33npLKSkp6tGjh+6++25PckWSbcKECaXbq2vXrmrZsmX0Q7lKpqioyHXs2NF9//33rqCgwPXs2dOtX7++zDKvvPKKGz9+vHPOuczMTDdy5EjfZNu8ebMLBoNuzJgx7u233/ZNrpycHBcKhZxzzs2dO9dX2+zAgQOlt7Oystwtt9zii1xHsw0aNMilp6e7L774Iuq5Is22YMEC9/DDD3uS56fkys3NdWlpaW7v3r3OOed27tzpm2zHmj17trv//vujnqvS7QFHcipzdna2+vTpI0nq2rWrcnJy5Dx4rzGSbA0aNNDll1+uKlW82/SR5GrTpo2qV68u6ciJL8d+Vts6W40aNUpvHzp0SDExMb7IJUnPPfecbrvtNk/P1PTr6fyR5Hr99dd14403qnbt2pKkunXr+ibbsZYsWaLU1NSo56p0Bbx9+3bVr1+/9H5iYqK2b99+3DK/+MUvJB352FvNmjW1Z88eX2Sz8FNzzZ8/X+3bt/ciWsTZ5s6dq06dOunpp5/Wgw8+6ItcX331lbZt26bf//73Uc/zU7NJ0jvvvKOePXsqIyNDW7du9UWujRs3Kjc3VwMHDtSAAQO0YsWKqOeKNNtRW7ZsUV5entq0aRP1XJWugBFdixcv1pdffqlhw4ZZRynjxhtvVFZWlu655x5NnTrVOo5KSkr0xBNP6L777rOOckIdOnRQdna23nzzTV177bW+yVlcXKxNmzZpzpw5euaZZzR+/Hjt37/fOlYZS5YsUdeuXVW1atWoz1XpCjiSU5kTExNL/+IXFRXpwIEDOv/8832RzUKkuVauXKkXX3xRU6dOVVxcnK+yHdWjRw9lZWWZ58rPz9c333yjP/zhD0pOTtZnn32mP/7xj568ERfJNjv//PNLn8P09HR99dVXvsiVmJio5ORkVatWTQ0bNtSll16qjRs3+iLbUW+99ZZ69OgR9UxSJSzgSE5lTk5O1htvvCHpyDvUbdq08eS4oV9Ps44k17p16/TQQw9p6tSpnh2XizTbsS/Q5cuX65JLLjHPVbNmTa1evVrZ2dnKzs5WixYtNHXqVF155ZXm2SRpx44dpbezs7PVuHFjX+Tq1KmTPvzwQ0nS7t27tXHjRjVs2NAX2SRpw4YN2r9/v37zm99EPZOkyvcpCOecW758uevSpYvr2LGje+GFF5xzzj377LMuKyvLOedcOBx2I0aMcJ06dXL9+vVz33//vW+yff755+66665zzZs3d61bt3YpKSm+yDVkyBCXlJTkevXq5Xr16uVuv/12T3JFku3RRx91KSkprlevXm7w4MHum2++8UWuYw0ePNizT0FEkm3SpEkuJSXF9ezZ0w0ePNh9++23vshVUlLiJkyY4Lp37+5SU1NdZmamJ7kiyeacc1OmTHFPP/20Z5k4FRkAjFS6QxAAcKaggAHACAUMAEYoYAAwQgEDgBEKGKdFIBBQWlqaUlNTlZGRoUOHDllHMnXTTTed8qSMl19+ucx2uu2223x3VhiiiwLGaXHOOedo8eLFyszMVLVq1fTaa6/9rPUVFRWdpmSnT3Fxcbn3f6rZs2eXKeCXXnpJtWrV+lnrROVSKS5Lj8qlZcuW+vrrrxUKhfToo49q/fr1Kioq0l133aVOnTopLy9P9957b2n5jB8/XldffbVWr16t5557TrVq1VJubq7eeOMNjRo1Stu2bVNJSYnuvPNOpaSkKCcnR08++aSKi4vVrFkzPfzww4qLi1NycrJ69+6td999V0VFRXr22WePOwOsuLhYkyZN0vvvv6+YmBgNGDBAN910U7nr7N69u1auXKlhw4bpmWeeKXO/du3aev7551VYWKiGDRtq4sSJOvfcc8vM+ec//1lr165VQUGBunbtqoyMDM2ePVs7duzQkCFDdN5552nOnDlKTk7W/PnzVadOHc2aNUsLFiyQJPXv318333yz8vLydNttt+m3v/2t1qxZo8TERL3wwgs655xzvHlicfp5dsoHzmgtWrRwzjl3+PBhd8cdd7i5c+e6Z555xi1atMg559y+fftcly5dXH5+vguFQi4cDjvnjnw/bJ8+fZxzzq1atco1b9689MzFpUuXugceeKB0jv3797twOOzat2/vvvvuO+ecc2PGjHGzZs1yzjnXoUMHN3v2bOfcke+EHjdu3HE5586d60aMGOEOHz7snHNuz549p1zn9OnTSx9/7P1du3a5QYMGufz8fOecc9OmTXPPP/+8c67smXF79uxxzh35TtrBgwe7YDBYuq5du3aVWfeuXbvc2rVrXWpqqsvPz3cHDx50KSkp7quvvnKbN292gUDArVu3zjnnXEZGRun2ReXEIQicFuFwWGlpaerXr58uuugi9e/fX//+97/10ksvKS0tTTfddJMKCgq0detWFRUV6cEHH1TPnj01cuRIbdiwoXQ9V155Zel3A/z617/WypUr9fTTT+vjjz9WzZo1lZubqwYNGuiyyy6TJPXp00cff/xx6eO7dOkiSWrWrNkJr+qRk5Oj66+/XrGxR/7n77zzzjvlOlNSUsqs4+j9zz//XN9++61uuOEGpaWladGiRfrvf/973Jxvv/22+vTpo969e2v9+vVlft8T+eSTT9SpUyclJCTo3HPPVefOnUvzNGjQQIFAQJLUtGlTT69cgtOPQxA4LY4eA/6xKVOmqFGjRmXGnn/+eV1wwQVavHixSkpKdNVVV5X+LCEhofT2ZZddpoULF+q9997Ts88+qzZt2qhTp07l5qhWrZokqUqVKj/7GO1RR7+o/sf3nXNq27atJk+efNLHbt68WTNnztT8+fNVu3Zt3X///SooKKhwlmO/pa5q1ao/a12wxx4woqZdu3Z65ZVXSq9Gsm7dOknSgQMHdOGFF6pKlSpavHjxSYty+/btql69utLS0nTrrbdq3bp1uuyyy7RlyxZt2rRJ0pHvL27VqlXEma699lr94x//KH2Tb+/evRVeZ4sWLfTpp5+WPi4UCik3N7fMMvn5+apevbpq1qypnTt3lvkC8nPPPVf5+fnHrbdly5bKysrSoUOHFAqFlJWV5c31yeA59oARNXfeeacmTJigXr16qaSkRA0aNNC0adM0aNAgjRgxQosWLdJ1111XZq/3WN98842eeuopValSRbGxsfrLX/6i+Ph4TZw4USNHjix9w+yGG26IOFN6ero2btyoXr16KTY2VgMGDNDgwYMrtM46depo4sSJGj16tAoLCyVJo0aNKj2UIUmXX365rrjiCnXv3l3169fX1VdfXfqzAQMGaNiwYapXr57mzJlTOt60aVP17dtX6enpko68CXfFFVd4enFZeINvQwMAIxyCAAAjFDAAGKGAAcAIBQwARihgADBCAQOAEQoYAIz8P2NITtNyxvjYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = pearson_correlation_score(d.x_target.cpu().numpy(), x_pred, sample_corr=True)\n",
    "sns.displot(scores)\n",
    "plt.xlabel('Pearson correlation')\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c9c03",
   "metadata": {},
   "source": [
    "Not bad! These are per-sample scores. Let's see if we can improve the predictions by taking multiple tissues as reference for the same validation individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c7b70ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 168 source and 42 target samples of 42 unique donors\n"
     ]
    }
   ],
   "source": [
    "source_tissues_2 = ['Whole_Blood', 'Skin_Sun_Epsd', 'Skin_Not_Sun_Epsd', 'Adipose_Subcutaneous']\n",
    "aux_val_dataset_2 = HypergraphDataset(adata[split_mask],\n",
    "                                    obs_source={'Tissue': source_tissues_2, 'Participant ID': valid_donors},\n",
    "                                    obs_target={'Tissue': target_tissues, 'Participant ID': valid_donors},\n",
    "                                    static=True, \n",
    "                                    verbose=True)\n",
    "aux_val_loader_2 = DataLoader(aux_val_dataset_2,\n",
    "                            batch_size=len(aux_val_dataset_2),\n",
    "                            collate_fn=Data.from_datalist,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dade3e",
   "metadata": {},
   "source": [
    "Let's feed the data (accessible tissues) through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f17e59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(aux_val_loader_2))\n",
    "with torch.no_grad():\n",
    "    out, node_features = forward(d, model, device, preprocess_fn=None)\n",
    "    x_pred = out['px_rate'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d1991",
   "metadata": {},
   "source": [
    "The average prediction performance are now higher (though the predictions are now worse for a couple of individuals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7cb6bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4354319, 0.18939544)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = pearson_correlation_score(d.x_target.cpu().numpy(), x_pred, sample_corr=True)\n",
    "sns.displot(scores)\n",
    "plt.xlabel('Pearson correlation')\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e83196",
   "metadata": {},
   "source": [
    "Well done! As an exercise, you may try to impute the gene expression of the target tissue using all collected tissues of the individual. Think about how you could set up the auxiliary dataset to select the appropriate samples (note that different individuals may have different collected tissues)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae9712",
   "metadata": {},
   "source": [
    "---\n",
    "## Plotting the HYFA's learnt tissue embeddings\n",
    "\n",
    "In this section, we explore HYFA's learnt tissue embeddings (i.e. learnable node features in the hypergraph). We gather the learnt tissue features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1439cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.params['Tissue'].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02143c4",
   "metadata": {},
   "source": [
    "We will now apply UMAP to project these parameters into a 2-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43340522",
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_params_2d = umap.UMAP().fit_transform(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148f07d",
   "metadata": {},
   "source": [
    "We finally plot the low-dimensional tissue embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.gca()\n",
    "x1, x2 = tissue_params_2d.T\n",
    "ax.scatter(x1, x2,\n",
    "           c=adata.uns['Tissue_colors'],\n",
    "           s=300)\n",
    "\n",
    "for t, i in adata.uns['Tissue_dict'].items():\n",
    "    x_coord = x1[i]\n",
    "    y_coord = x2[i]\n",
    "    txt = t.replace('_', ' ').replace('Brain', '')\n",
    "        \n",
    "    ax.annotate(txt, (x_coord, y_coord),\n",
    "                textcoords=\"offset points\",  # how to position the text\n",
    "                xytext=(0, 10),  # distance from text to points (x,y)\n",
    "                fontsize=12,\n",
    "                ha='center')\n",
    "\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952ed7f",
   "metadata": {},
   "source": [
    "We observe strong cluster of biologically related tissues (e.g. see brain tissues in yellow). Cool!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multitissue",
   "language": "python",
   "name": "multitissue"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
